{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e2f66a",
   "metadata": {},
   "source": [
    "# Hands-On: Ekstraksi dan Transformasi Data Sensor dengan Pandas\n",
    "\n",
    "## Deskripsi\n",
    "Tutorial ini akan memandu Anda melalui proses **Extract, Transform, Load (ETL)** untuk data sensor menggunakan Python Pandas. Anda akan belajar:\n",
    "\n",
    "- 🔄 **Ekstraksi**: Membaca data dari berbagai format (CSV, Excel, JSON)\n",
    "- 🧹 **Transformasi**: Pembersihan, preprocessing, dan feature engineering\n",
    "- 💾 **Loading**: Menyimpan hasil ke berbagai format output\n",
    "\n",
    "## Dataset\n",
    "Data yang digunakan adalah simulasi pembacaan sensor IoT yang mencakup:\n",
    "- **Suhu** (°C)\n",
    "- **Kelembaban** (%)  \n",
    "- **Tekanan Atmosfer** (hPa)\n",
    "- **Kualitas Udara** (AQI)\n",
    "\n",
    "## Prerequisites\n",
    "Pastikan Anda sudah menginstall packages yang dibutuhkan:\n",
    "```bash\n",
    "pip install pandas numpy matplotlib seaborn openpyxl plotly\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2c16d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Mari mulai dengan mengimport semua library yang diperlukan untuk proses ETL data sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries untuk data manipulation dan analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Libraries untuk visualisasi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set style untuk plot\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Konfigurasi pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"✅ Libraries berhasil diimport!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ffd55",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Data (Opsional)\n",
    "Jika belum ada data sample, jalankan script berikut untuk generate data sensor simulasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jalankan script untuk generate sample data\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, 'scripts/generate_sample_data.py'], \n",
    "                          capture_output=True, text=True, cwd='/Users/ekosakti/Code/PID/ETL-Pandas')\n",
    "    print(\"📊 Sample data generation:\")\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"⚠️ Warnings/Errors:\")\n",
    "        print(result.stderr)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error running script: {e}\")\n",
    "    print(\"💡 Silakan jalankan secara manual: python scripts/generate_sample_data.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea20cd3",
   "metadata": {},
   "source": [
    "## 3. Load Sensor Data (EXTRACT)\n",
    "Mari mulai proses ekstraksi data dari berbagai sumber dan format yang berbeda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Load data dari CSV\n",
    "print(\"🔄 Loading data dari berbagai format...\")\n",
    "\n",
    "# Load main CSV data\n",
    "try:\n",
    "    df_main = pd.read_csv('data/raw/sensor_data_main.csv')\n",
    "    print(f\"✅ CSV Main Data: {df_main.shape[0]} rows, {df_main.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File sensor_data_main.csv tidak ditemukan. Jalankan generate_sample_data.py terlebih dahulu.\")\n",
    "    df_main = pd.DataFrame()\n",
    "\n",
    "# Load CSV dengan separator semicolon\n",
    "try:\n",
    "    df_semicolon = pd.read_csv('data/raw/sensor_data_semicolon.csv', sep=';')\n",
    "    print(f\"✅ CSV Semicolon Data: {df_semicolon.shape[0]} rows, {df_semicolon.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File sensor_data_semicolon.csv tidak ditemukan.\")\n",
    "    df_semicolon = pd.DataFrame()\n",
    "\n",
    "# Load Excel data\n",
    "try:\n",
    "    excel_file = pd.ExcelFile('data/raw/sensor_data_monthly.xlsx')\n",
    "    print(f\"✅ Excel file dengan sheets: {excel_file.sheet_names}\")\n",
    "    \n",
    "    df_jan = pd.read_excel('data/raw/sensor_data_monthly.xlsx', sheet_name='January')\n",
    "    df_feb = pd.read_excel('data/raw/sensor_data_monthly.xlsx', sheet_name='February')\n",
    "    print(f\"   - January: {df_jan.shape[0]} rows\")\n",
    "    print(f\"   - February: {df_feb.shape[0]} rows\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File sensor_data_monthly.xlsx tidak ditemukan.\")\n",
    "    df_jan = df_feb = pd.DataFrame()\n",
    "\n",
    "# Load JSON config\n",
    "try:\n",
    "    with open('data/raw/sensor_config.json', 'r') as f:\n",
    "        sensor_config = json.load(f)\n",
    "    print(f\"✅ JSON Config: {len(sensor_config)} sensor configurations loaded\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File sensor_config.json tidak ditemukan.\")\n",
    "    sensor_config = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bdb6d1",
   "metadata": {},
   "source": [
    "## 4. Data Exploration and Information (EXAMINE)\n",
    "Sebelum melakukan transformasi, mari kita explore dan pahami struktur data yang telah dimuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Basic Information\n",
    "if not df_main.empty:\n",
    "    print(\"📊 DATASET OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Shape: {df_main.shape}\")\n",
    "    print(f\"Memory usage: {df_main.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print()\n",
    "    \n",
    "    print(\"📋 COLUMN INFO\")\n",
    "    print(\"=\" * 50)\n",
    "    print(df_main.info())\n",
    "    print()\n",
    "    \n",
    "    print(\"🔢 BASIC STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(df_main.describe())\n",
    "    print()\n",
    "    \n",
    "    print(\"👀 SAMPLE DATA\")\n",
    "    print(\"=\" * 50)\n",
    "    print(df_main.head())\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Data utama kosong. Pastikan file CSV sudah di-generate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec63814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Data Quality Assessment\n",
    "if not df_main.empty:\n",
    "    print(\"🔍 DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Missing values\n",
    "    missing_data = df_main.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df_main)) * 100\n",
    "    \n",
    "    quality_df = pd.DataFrame({\n",
    "        'Column': missing_data.index,\n",
    "        'Missing_Count': missing_data.values,\n",
    "        'Missing_Percent': missing_percent.values,\n",
    "        'Data_Type': df_main.dtypes.values\n",
    "    })\n",
    "    \n",
    "    print(\"📊 Missing Values Analysis:\")\n",
    "    print(quality_df[quality_df['Missing_Count'] > 0])\n",
    "    print()\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df_main.duplicated().sum()\n",
    "    print(f\"🔁 Duplicate rows: {duplicates}\")\n",
    "    \n",
    "    # Unique values per column\n",
    "    print(\"\\n🏷️ Unique Values per Column:\")\n",
    "    for col in df_main.columns:\n",
    "        unique_count = df_main[col].nunique()\n",
    "        print(f\"   {col}: {unique_count} unique values\")\n",
    "    \n",
    "    # Data range validation\n",
    "    print(\"\\n📏 Data Range Validation:\")\n",
    "    numeric_cols = df_main.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'sensor_id':\n",
    "            min_val = df_main[col].min()\n",
    "            max_val = df_main[col].max()\n",
    "            print(f\"   {col}: {min_val:.2f} to {max_val:.2f}\")\n",
    "            \n",
    "            # Check for obvious outliers (beyond realistic sensor ranges)\n",
    "            if 'temperature' in col:\n",
    "                outliers = df_main[(df_main[col] < -50) | (df_main[col] > 60)][col].count()\n",
    "                print(f\"      Potential outliers (< -50°C or > 60°C): {outliers}\")\n",
    "            elif 'humidity' in col:\n",
    "                outliers = df_main[(df_main[col] < 0) | (df_main[col] > 100)][col].count()\n",
    "                print(f\"      Potential outliers (< 0% or > 100%): {outliers}\")\n",
    "            elif 'pressure' in col:\n",
    "                outliers = df_main[(df_main[col] < 900) | (df_main[col] > 1100)][col].count()\n",
    "                print(f\"      Potential outliers (< 900 hPa or > 1100 hPa): {outliers}\")\n",
    "else:\n",
    "    print(\"❌ Data utama kosong untuk quality assessment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1215d",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning and Preprocessing (TRANSFORM - Part 1)\n",
    "Mari mulai proses transformasi dengan membersihkan data dari berbagai masalah kualitas yang ditemukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Create working copy\n",
    "if not df_main.empty:\n",
    "    df_clean = df_main.copy()\n",
    "    print(f\"📋 Working dengan dataset copy: {df_clean.shape}\")\n",
    "    print()\n",
    "    \n",
    "    # 5.2 Handle duplicates\n",
    "    print(\"🔁 REMOVING DUPLICATES\")\n",
    "    print(\"=\" * 30)\n",
    "    before_dup = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    after_dup = len(df_clean)\n",
    "    removed_dup = before_dup - after_dup\n",
    "    print(f\"Duplicate rows removed: {removed_dup}\")\n",
    "    print(f\"Remaining rows: {after_dup}\")\n",
    "    print()\n",
    "    \n",
    "    # 5.3 Standardize column names\n",
    "    print(\"📝 STANDARDIZING COLUMN NAMES\")\n",
    "    print(\"=\" * 35)\n",
    "    original_cols = df_clean.columns.tolist()\n",
    "    \n",
    "    # Clean column names (lowercase, replace spaces with underscores)\n",
    "    df_clean.columns = [col.lower().replace(' ', '_').replace('-', '_') for col in df_clean.columns]\n",
    "    \n",
    "    renamed_cols = df_clean.columns.tolist()\n",
    "    print(\"Column name changes:\")\n",
    "    for old, new in zip(original_cols, renamed_cols):\n",
    "        if old != new:\n",
    "            print(f\"  '{old}' → '{new}'\")\n",
    "    print()\n",
    "    \n",
    "    # 5.4 Standardize categorical data\n",
    "    print(\"🏷️ STANDARDIZING CATEGORICAL DATA\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Standardize location names\n",
    "    if 'location' in df_clean.columns:\n",
    "        # Convert to consistent format (Title Case)\n",
    "        df_clean['location'] = df_clean['location'].str.title()\n",
    "        print(f\"Location values: {df_clean['location'].unique()}\")\n",
    "    \n",
    "    # Standardize status\n",
    "    if 'status' in df_clean.columns:\n",
    "        df_clean['status'] = df_clean['status'].str.lower().str.strip()\n",
    "        print(f\"Status values: {df_clean['status'].unique()}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"✅ Basic cleaning completed!\")\n",
    "    print(f\"Final dataset shape: {df_clean.shape}\")\n",
    "else:\n",
    "    print(\"❌ Tidak dapat melakukan cleaning - dataset kosong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8194a6d5",
   "metadata": {},
   "source": [
    "## 6. Time Series Data Handling (TRANSFORM - Part 2)\n",
    "Karena data sensor adalah time series, kita perlu menangani aspek temporal dengan benar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Convert timestamp to datetime\n",
    "if not df_clean.empty and 'timestamp' in df_clean.columns:\n",
    "    print(\"📅 TIME SERIES PROCESSING\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])\n",
    "    print(f\"✅ Converted timestamp to datetime\")\n",
    "    print(f\"Date range: {df_clean['timestamp'].min()} to {df_clean['timestamp'].max()}\")\n",
    "    print()\n",
    "    \n",
    "    # Extract time components\n",
    "    print(\"🕐 EXTRACTING TIME COMPONENTS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    df_clean['year'] = df_clean['timestamp'].dt.year\n",
    "    df_clean['month'] = df_clean['timestamp'].dt.month\n",
    "    df_clean['day'] = df_clean['timestamp'].dt.day\n",
    "    df_clean['hour'] = df_clean['timestamp'].dt.hour\n",
    "    df_clean['day_of_week'] = df_clean['timestamp'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df_clean['day_name'] = df_clean['timestamp'].dt.day_name()\n",
    "    df_clean['is_weekend'] = df_clean['day_of_week'].isin([5, 6])  # Saturday, Sunday\n",
    "    \n",
    "    # Create time periods\n",
    "    df_clean['time_period'] = pd.cut(df_clean['hour'], \n",
    "                                   bins=[0, 6, 12, 18, 24], \n",
    "                                   labels=['Night', 'Morning', 'Afternoon', 'Evening'],\n",
    "                                   include_lowest=True)\n",
    "    \n",
    "    print(\"Added time features:\")\n",
    "    print(f\"  - year, month, day, hour\")\n",
    "    print(f\"  - day_of_week, day_name\")\n",
    "    print(f\"  - is_weekend (boolean)\")\n",
    "    print(f\"  - time_period (categorical)\")\n",
    "    print()\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df_clean = df_clean.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(\"✅ Data sorted by timestamp\")\n",
    "    \n",
    "    # Display time feature examples\n",
    "    time_features = ['timestamp', 'year', 'month', 'day', 'hour', 'day_name', 'time_period', 'is_weekend']\n",
    "    available_features = [col for col in time_features if col in df_clean.columns]\n",
    "    print(\"\\\\n📊 Sample time features:\")\n",
    "    print(df_clean[available_features].head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Tidak dapat memproses timestamp - data kosong atau kolom timestamp tidak ada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f0a632",
   "metadata": {},
   "source": [
    "## 7. Data Filtering and Selection (TRANSFORM - Part 3)\n",
    "Mari pelajari berbagai teknik untuk memfilter dan menseleksi data berdasarkan kondisi tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Various filtering techniques\n",
    "if not df_clean.empty:\n",
    "    print(\"🔍 DATA FILTERING EXAMPLES\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # 7.1.1 Filter by sensor conditions\n",
    "    print(\"1️⃣ Filter by Temperature (Hot days > 30°C):\")\n",
    "    if 'temperature_celsius' in df_clean.columns:\n",
    "        hot_days = df_clean[df_clean['temperature_celsius'] > 30]\n",
    "        print(f\"   Records with temperature > 30°C: {len(hot_days)}\")\n",
    "        if len(hot_days) > 0:\n",
    "            print(f\"   Temperature range in hot days: {hot_days['temperature_celsius'].min():.2f}°C to {hot_days['temperature_celsius'].max():.2f}°C\")\n",
    "    print()\n",
    "    \n",
    "    # 7.1.2 Filter by time range\n",
    "    print(\"2️⃣ Filter by Time Range (Business hours 9-17):\")\n",
    "    if 'hour' in df_clean.columns:\n",
    "        business_hours = df_clean[(df_clean['hour'] >= 9) & (df_clean['hour'] <= 17)]\n",
    "        print(f\"   Records during business hours: {len(business_hours)}\")\n",
    "    print()\n",
    "    \n",
    "    # 7.1.3 Filter by location\n",
    "    print(\"3️⃣ Filter by Location:\")\n",
    "    if 'location' in df_clean.columns:\n",
    "        locations = df_clean['location'].unique()\n",
    "        print(f\"   Available locations: {locations}\")\n",
    "        if len(locations) > 0:\n",
    "            first_location = locations[0]\n",
    "            location_data = df_clean[df_clean['location'] == first_location]\n",
    "            print(f\"   Records for {first_location}: {len(location_data)}\")\n",
    "    print()\n",
    "    \n",
    "    # 7.1.4 Multiple condition filtering\n",
    "    print(\"4️⃣ Complex Filtering (High temperature AND high humidity):\")\n",
    "    if all(col in df_clean.columns for col in ['temperature_celsius', 'humidity_percent']):\n",
    "        complex_filter = df_clean[\n",
    "            (df_clean['temperature_celsius'] > 25) & \n",
    "            (df_clean['humidity_percent'] > 70)\n",
    "        ]\n",
    "        print(f\"   Records with temp > 25°C AND humidity > 70%: {len(complex_filter)}\")\n",
    "    print()\n",
    "    \n",
    "    # 7.1.5 Using query method\n",
    "    print(\"5️⃣ Using .query() method (Weekend data):\")\n",
    "    if 'is_weekend' in df_clean.columns:\n",
    "        weekend_data = df_clean.query('is_weekend == True')\n",
    "        print(f\"   Weekend records: {len(weekend_data)}\")\n",
    "    print()\n",
    "    \n",
    "    # 7.1.6 Filter by sensor status\n",
    "    print(\"6️⃣ Filter by Sensor Status:\")\n",
    "    if 'status' in df_clean.columns:\n",
    "        active_sensors = df_clean[df_clean['status'] == 'active']\n",
    "        maintenance_sensors = df_clean[df_clean['status'] == 'maintenance']\n",
    "        print(f\"   Active sensor records: {len(active_sensors)}\")\n",
    "        print(f\"   Maintenance sensor records: {len(maintenance_sensors)}\")\n",
    "    print()\n",
    "    \n",
    "    # 7.2 Date range filtering\n",
    "    print(\"📅 DATE RANGE FILTERING\")\n",
    "    print(\"=\" * 25)\n",
    "    if 'timestamp' in df_clean.columns:\n",
    "        # Last 7 days of data\n",
    "        max_date = df_clean['timestamp'].max()\n",
    "        week_ago = max_date - timedelta(days=7)\n",
    "        recent_data = df_clean[df_clean['timestamp'] >= week_ago]\n",
    "        print(f\"Last 7 days of data: {len(recent_data)} records\")\n",
    "        \n",
    "        # Specific month\n",
    "        january_data = df_clean[df_clean['timestamp'].dt.month == 1]\n",
    "        print(f\"January data: {len(january_data)} records\")\n",
    "    \n",
    "    print(\"\\\\n✅ Filtering examples completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Tidak dapat melakukan filtering - dataset kosong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14990fd",
   "metadata": {},
   "source": [
    "## 8. Data Transformation and Feature Engineering (TRANSFORM - Part 4)\n",
    "Mari buat feature baru dan transformasi data untuk mendapatkan insights yang lebih baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Feature Engineering\n",
    "if not df_clean.empty:\n",
    "    print(\"🔧 FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    # 8.1.1 Temperature conversions\n",
    "    if 'temperature_celsius' in df_clean.columns:\n",
    "        print(\"🌡️ Temperature Conversions:\")\n",
    "        df_clean['temperature_fahrenheit'] = (df_clean['temperature_celsius'] * 9/5) + 32\n",
    "        df_clean['temperature_kelvin'] = df_clean['temperature_celsius'] + 273.15\n",
    "        print(\"   ✅ Added Fahrenheit and Kelvin temperatures\")\n",
    "    \n",
    "    # 8.1.2 Heat Index calculation (simplified)\n",
    "    if all(col in df_clean.columns for col in ['temperature_celsius', 'humidity_percent']):\n",
    "        print(\"\\\\n🔥 Heat Index Calculation:\")\n",
    "        # Simplified heat index formula (for temperatures in Fahrenheit)\n",
    "        temp_f = df_clean['temperature_fahrenheit']\n",
    "        rh = df_clean['humidity_percent']\n",
    "        \n",
    "        # Only calculate for temperatures >= 80°F (~27°C)\n",
    "        heat_index = np.where(\n",
    "            temp_f >= 80,\n",
    "            -42.379 + 2.04901523*temp_f + 10.14333127*rh - 0.22475541*temp_f*rh - \n",
    "            0.00683783*temp_f**2 - 0.05481717*rh**2 + 0.00122874*temp_f**2*rh + \n",
    "            0.00085282*temp_f*rh**2 - 0.00000199*temp_f**2*rh**2,\n",
    "            temp_f  # For cooler temperatures, use actual temperature\n",
    "        )\n",
    "        \n",
    "        df_clean['heat_index_f'] = heat_index\n",
    "        df_clean['heat_index_c'] = (heat_index - 32) * 5/9\n",
    "        print(\"   ✅ Added Heat Index (comfort measure)\")\n",
    "    \n",
    "    # 8.1.3 Air Quality Categories\n",
    "    if 'air_quality_aqi' in df_clean.columns:\n",
    "        print(\"\\\\n🌪️ Air Quality Categorization:\")\n",
    "        def categorize_aqi(aqi):\n",
    "            if pd.isna(aqi):\n",
    "                return 'Unknown'\n",
    "            elif aqi <= 50:\n",
    "                return 'Good'\n",
    "            elif aqi <= 100:\n",
    "                return 'Moderate'  \n",
    "            elif aqi <= 150:\n",
    "                return 'Unhealthy for Sensitive Groups'\n",
    "            elif aqi <= 200:\n",
    "                return 'Unhealthy'\n",
    "            elif aqi <= 300:\n",
    "                return 'Very Unhealthy'\n",
    "            else:\n",
    "                return 'Hazardous'\n",
    "        \n",
    "        df_clean['aqi_category'] = df_clean['air_quality_aqi'].apply(categorize_aqi)\n",
    "        print(\"   ✅ Added AQI categories\")\n",
    "        print(f\"   Categories: {df_clean['aqi_category'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # 8.1.4 Comfort Index\n",
    "    print(\"\\\\n😌 Comfort Index Creation:\")\n",
    "    if all(col in df_clean.columns for col in ['temperature_celsius', 'humidity_percent']):\n",
    "        # Simple comfort index based on temperature and humidity\n",
    "        def comfort_score(temp, humidity):\n",
    "            if pd.isna(temp) or pd.isna(humidity):\n",
    "                return np.nan\n",
    "            \n",
    "            # Ideal ranges: temp 20-24°C, humidity 40-60%\n",
    "            temp_score = 100 - abs(temp - 22) * 5  # Penalty increases with distance from 22°C\n",
    "            humidity_score = 100 - abs(humidity - 50) * 2  # Penalty increases with distance from 50%\n",
    "            \n",
    "            # Combine scores\n",
    "            comfort = (temp_score + humidity_score) / 2\n",
    "            return max(0, min(100, comfort))  # Clamp to 0-100\n",
    "        \n",
    "        df_clean['comfort_index'] = df_clean.apply(\n",
    "            lambda row: comfort_score(row['temperature_celsius'], row['humidity_percent']), \n",
    "            axis=1\n",
    "        )\n",
    "        print(\"   ✅ Added Comfort Index (0-100 scale)\")\n",
    "    \n",
    "    # 8.1.5 Sensor performance indicators\n",
    "    print(\"\\\\n📊 Sensor Performance Indicators:\")\n",
    "    \n",
    "    # Group by sensor to calculate performance metrics\n",
    "    if 'sensor_id' in df_clean.columns:\n",
    "        sensor_stats = df_clean.groupby('sensor_id').agg({\n",
    "            'timestamp': 'count',\n",
    "            'temperature_celsius': ['mean', 'std'],\n",
    "            'humidity_percent': ['mean', 'std'],\n",
    "            'status': lambda x: (x == 'active').mean()\n",
    "        }).round(2)\n",
    "        \n",
    "        sensor_stats.columns = ['reading_count', 'avg_temp', 'temp_std', 'avg_humidity', 'humidity_std', 'uptime_ratio']\n",
    "        \n",
    "        # Merge back to main dataframe\n",
    "        df_clean = df_clean.merge(\n",
    "            sensor_stats[['reading_count', 'uptime_ratio']], \n",
    "            left_on='sensor_id', \n",
    "            right_index=True, \n",
    "            suffixes=('', '_sensor')\n",
    "        )\n",
    "        \n",
    "        print(f\"   ✅ Added sensor performance metrics\")\n",
    "        print(\"   Sample sensor stats:\")\n",
    "        print(sensor_stats.head())\n",
    "    \n",
    "    print(\"\\\\n🎯 NORMALIZATION AND SCALING\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # 8.2 Normalize numeric features (0-1 scale)\n",
    "    numeric_cols = ['temperature_celsius', 'humidity_percent', 'pressure_hpa', 'air_quality_aqi']\n",
    "    available_numeric = [col for col in numeric_cols if col in df_clean.columns]\n",
    "    \n",
    "    for col in available_numeric:\n",
    "        col_min = df_clean[col].min()\n",
    "        col_max = df_clean[col].max()\n",
    "        df_clean[f'{col}_normalized'] = (df_clean[col] - col_min) / (col_max - col_min)\n",
    "    \n",
    "    print(f\"✅ Normalized columns: {available_numeric}\")\n",
    "    \n",
    "    print(f\"\\\\n📈 Total features after engineering: {len(df_clean.columns)}\")\n",
    "    print(\"New feature columns:\")\n",
    "    new_features = [col for col in df_clean.columns if any(x in col for x in \n",
    "                    ['fahrenheit', 'kelvin', 'heat_index', 'comfort', 'normalized', 'aqi_category', 'uptime', 'reading_count'])]\n",
    "    for feature in new_features:\n",
    "        print(f\"   - {feature}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Tidak dapat melakukan feature engineering - dataset kosong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba70387c",
   "metadata": {},
   "source": [
    "## 9. Data Aggregation and Grouping (TRANSFORM - Part 5)\n",
    "Sekarang mari pelajari bagaimana mengelompokkan dan mengagregasi data sensor untuk mendapatkan insights yang meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Time-based aggregations\n",
    "if not df_clean.empty:\n",
    "    print(\"📊 TIME-BASED AGGREGATIONS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # 9.1.1 Hourly aggregation\n",
    "    if 'timestamp' in df_clean.columns:\n",
    "        print(\"⏰ Hourly Aggregation:\")\n",
    "        hourly_data = df_clean.groupby(df_clean['timestamp'].dt.floor('H')).agg({\n",
    "            'temperature_celsius': ['mean', 'min', 'max', 'std'],\n",
    "            'humidity_percent': ['mean', 'min', 'max'],\n",
    "            'pressure_hpa': 'mean',\n",
    "            'air_quality_aqi': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten column names\n",
    "        hourly_data.columns = ['_'.join(col).strip() for col in hourly_data.columns]\n",
    "        hourly_data.reset_index(inplace=True)\n",
    "        \n",
    "        print(f\"   Records aggregated to hourly: {len(hourly_data)}\")\n",
    "        print(\"   Sample hourly data:\")\n",
    "        print(hourly_data.head(3))\n",
    "        print()\n",
    "    \n",
    "    # 9.1.2 Daily aggregation\n",
    "    print(\"📅 Daily Aggregation:\")\n",
    "    if 'timestamp' in df_clean.columns:\n",
    "        daily_data = df_clean.groupby(df_clean['timestamp'].dt.date).agg({\n",
    "            'temperature_celsius': ['mean', 'min', 'max'],\n",
    "            'humidity_percent': ['mean', 'min', 'max'],\n",
    "            'pressure_hpa': ['mean', 'std'],\n",
    "            'air_quality_aqi': ['mean', 'max'],\n",
    "            'sensor_id': 'nunique'  # Number of active sensors per day\n",
    "        }).round(2)\n",
    "        \n",
    "        daily_data.columns = ['_'.join(col).strip() for col in daily_data.columns]\n",
    "        daily_data.reset_index(inplace=True)\n",
    "        \n",
    "        print(f\"   Records aggregated to daily: {len(daily_data)}\")\n",
    "        print(\"   Sample daily data:\")\n",
    "        print(daily_data.head(3))\n",
    "        print()\n",
    "    \n",
    "    # 9.2 Location-based aggregations  \n",
    "    print(\"🗺️ LOCATION-BASED AGGREGATIONS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    if 'location' in df_clean.columns:\n",
    "        location_summary = df_clean.groupby('location').agg({\n",
    "            'temperature_celsius': ['count', 'mean', 'min', 'max', 'std'],\n",
    "            'humidity_percent': ['mean', 'std'],\n",
    "            'air_quality_aqi': ['mean', 'max'],\n",
    "            'comfort_index': 'mean' if 'comfort_index' in df_clean.columns else 'count'\n",
    "        }).round(2)\n",
    "        \n",
    "        location_summary.columns = ['_'.join(col).strip() for col in location_summary.columns]\n",
    "        \n",
    "        print(\"Location Summary:\")\n",
    "        print(location_summary)\n",
    "        print()\n",
    "    \n",
    "    # 9.3 Sensor-based aggregations\n",
    "    print(\"🔧 SENSOR-BASED AGGREGATIONS\")\n",
    "    print(\"=\" * 32)\n",
    "    \n",
    "    if 'sensor_id' in df_clean.columns:\n",
    "        sensor_performance = df_clean.groupby('sensor_id').agg({\n",
    "            'temperature_celsius': ['count', 'mean', 'std'],\n",
    "            'humidity_percent': ['mean', 'std'], \n",
    "            'status': lambda x: (x == 'active').mean(),\n",
    "            'timestamp': lambda x: x.max() - x.min()  # Data collection span\n",
    "        }).round(3)\n",
    "        \n",
    "        sensor_performance.columns = ['reading_count', 'avg_temp', 'temp_variability', \n",
    "                                    'avg_humidity', 'humidity_variability', \n",
    "                                    'uptime_percentage', 'collection_span']\n",
    "        \n",
    "        print(\"Sensor Performance Summary:\")\n",
    "        print(sensor_performance.head())\n",
    "        print()\n",
    "    \n",
    "    # 9.4 Time period aggregations\n",
    "    print(\"🕐 TIME PERIOD AGGREGATIONS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    if 'time_period' in df_clean.columns:\n",
    "        period_analysis = df_clean.groupby('time_period').agg({\n",
    "            'temperature_celsius': 'mean',\n",
    "            'humidity_percent': 'mean',\n",
    "            'air_quality_aqi': 'mean',\n",
    "            'comfort_index': 'mean' if 'comfort_index' in df_clean.columns else 'count'\n",
    "        }).round(2)\n",
    "        \n",
    "        print(\"Average readings by time period:\")\n",
    "        print(period_analysis)\n",
    "        print()\n",
    "    \n",
    "    # 9.5 Complex multi-level grouping\n",
    "    print(\"🏢 MULTI-LEVEL GROUPING\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    if all(col in df_clean.columns for col in ['location', 'time_period']):\n",
    "        multi_group = df_clean.groupby(['location', 'time_period']).agg({\n",
    "            'temperature_celsius': 'mean',\n",
    "            'humidity_percent': 'mean',\n",
    "            'air_quality_aqi': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        print(\"Location vs Time Period Analysis:\")\n",
    "        print(multi_group.head(10))\n",
    "        print()\n",
    "    \n",
    "    # 9.6 Rolling aggregations (time series)\n",
    "    print(\"📈 ROLLING AGGREGATIONS\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    if 'timestamp' in df_clean.columns and len(df_clean) > 24:\n",
    "        # Sort by timestamp for rolling calculations\n",
    "        df_sorted = df_clean.sort_values('timestamp')\n",
    "        \n",
    "        # 24-hour rolling averages (assuming hourly data)\n",
    "        df_sorted['temp_rolling_24h'] = df_sorted['temperature_celsius'].rolling(window=24, min_periods=1).mean()\n",
    "        df_sorted['humidity_rolling_24h'] = df_sorted['humidity_percent'].rolling(window=24, min_periods=1).mean()\n",
    "        \n",
    "        print(\"✅ Added 24-hour rolling averages\")\n",
    "        print(\"Sample rolling data:\")\n",
    "        rolling_cols = ['timestamp', 'temperature_celsius', 'temp_rolling_24h', 'humidity_percent', 'humidity_rolling_24h']\n",
    "        available_rolling = [col for col in rolling_cols if col in df_sorted.columns]\n",
    "        print(df_sorted[available_rolling].head(10))\n",
    "    \n",
    "    print(\"\\\\n✅ Aggregation examples completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Tidak dapat melakukan aggregation - dataset kosong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3fd502",
   "metadata": {},
   "source": [
    "## 10. Handling Missing Values (TRANSFORM - Part 6)\n",
    "Data sensor sering memiliki missing values. Mari pelajari berbagai strategi untuk menanganinya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bede38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Analyze missing values patterns\n",
    "if not df_clean.empty:\n",
    "    print(\"🔍 MISSING VALUES ANALYSIS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Count missing values\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Column': df_clean.columns,\n",
    "        'Missing_Count': [df_clean[col].isnull().sum() for col in df_clean.columns],\n",
    "        'Missing_Percentage': [df_clean[col].isnull().sum() / len(df_clean) * 100 for col in df_clean.columns],\n",
    "        'Data_Type': df_clean.dtypes.values\n",
    "    })\n",
    "    \n",
    "    missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    if len(missing_summary) > 0:\n",
    "        print(\"Missing Values Summary:\")\n",
    "        print(missing_summary)\n",
    "        print()\n",
    "        \n",
    "        # Visualize missing values pattern\n",
    "        if len(missing_summary) <= 10:  # Only if manageable number of columns\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            missing_summary.plot(x='Column', y='Missing_Percentage', kind='bar', ax=ax)\n",
    "            ax.set_title('Missing Values by Column (%)')\n",
    "            ax.set_ylabel('Missing Percentage')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"✅ No missing values detected!\")\n",
    "        print()\n",
    "    \n",
    "    # 10.2 Different imputation strategies\n",
    "    print(\"🔧 MISSING VALUES TREATMENT\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Create a copy for imputation experiments\n",
    "    df_imputed = df_clean.copy()\n",
    "    \n",
    "    # Strategy 1: Forward Fill (for time series)\n",
    "    print(\"1️⃣ Forward Fill Strategy:\")\n",
    "    numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns\n",
    "    sensor_cols = [col for col in numeric_cols if any(x in col for x in ['temperature', 'humidity', 'pressure', 'aqi'])]\n",
    "    \n",
    "    if sensor_cols:\n",
    "        # Forward fill within each sensor\n",
    "        if 'sensor_id' in df_imputed.columns:\n",
    "            for sensor_col in sensor_cols:\n",
    "                before_ffill = df_imputed[sensor_col].isnull().sum()\n",
    "                df_imputed[sensor_col] = df_imputed.groupby('sensor_id')[sensor_col].fillna(method='ffill')\n",
    "                after_ffill = df_imputed[sensor_col].isnull().sum()\n",
    "                filled = before_ffill - after_ffill\n",
    "                if filled > 0:\n",
    "                    print(f\"   {sensor_col}: Filled {filled} values using forward fill\")\n",
    "    \n",
    "    # Strategy 2: Interpolation (for time series)\n",
    "    print(\"\\\\n2️⃣ Interpolation Strategy:\")\n",
    "    for sensor_col in sensor_cols:\n",
    "        if df_imputed[sensor_col].isnull().sum() > 0:\n",
    "            before_interp = df_imputed[sensor_col].isnull().sum()\n",
    "            \n",
    "            if 'sensor_id' in df_imputed.columns:\n",
    "                # Interpolate within each sensor group\n",
    "                df_imputed[sensor_col] = df_imputed.groupby('sensor_id')[sensor_col].transform(\n",
    "                    lambda x: x.interpolate(method='linear')\n",
    "                )\n",
    "            else:\n",
    "                df_imputed[sensor_col] = df_imputed[sensor_col].interpolate(method='linear')\n",
    "            \n",
    "            after_interp = df_imputed[sensor_col].isnull().sum()\n",
    "            filled = before_interp - after_interp\n",
    "            if filled > 0:\n",
    "                print(f\"   {sensor_col}: Filled {filled} values using interpolation\")\n",
    "    \n",
    "    # Strategy 3: Statistical imputation (mean/median by group)\n",
    "    print(\"\\\\n3️⃣ Statistical Imputation:\")\n",
    "    for sensor_col in sensor_cols:\n",
    "        if df_imputed[sensor_col].isnull().sum() > 0:\n",
    "            before_stat = df_imputed[sensor_col].isnull().sum()\n",
    "            \n",
    "            if 'location' in df_imputed.columns:\n",
    "                # Use location-based mean\n",
    "                location_means = df_imputed.groupby('location')[sensor_col].mean()\n",
    "                df_imputed[sensor_col] = df_imputed[sensor_col].fillna(\n",
    "                    df_imputed['location'].map(location_means)\n",
    "                )\n",
    "            else:\n",
    "                # Use overall mean\n",
    "                overall_mean = df_imputed[sensor_col].mean()\n",
    "                df_imputed[sensor_col] = df_imputed[sensor_col].fillna(overall_mean)\n",
    "            \n",
    "            after_stat = df_imputed[sensor_col].isnull().sum()\n",
    "            filled = before_stat - after_stat\n",
    "            if filled > 0:\n",
    "                print(f\"   {sensor_col}: Filled {filled} values using statistical imputation\")\n",
    "    \n",
    "    # Strategy 4: Domain-specific imputation\n",
    "    print(\"\\\\n4️⃣ Domain-Specific Imputation:\")\n",
    "    \n",
    "    # For categorical variables like status\n",
    "    if 'status' in df_imputed.columns and df_imputed['status'].isnull().sum() > 0:\n",
    "        before_status = df_imputed['status'].isnull().sum()\n",
    "        # Assume missing status means 'active'\n",
    "        df_imputed['status'] = df_imputed['status'].fillna('active')\n",
    "        print(f\"   status: Filled {before_status} missing status with 'active'\")\n",
    "    \n",
    "    # For air quality categories\n",
    "    if 'aqi_category' in df_imputed.columns and df_imputed['aqi_category'].isnull().sum() > 0:\n",
    "        before_aqi_cat = df_imputed['aqi_category'].isnull().sum()\n",
    "        # Re-derive from AQI values if available\n",
    "        if 'air_quality_aqi' in df_imputed.columns:\n",
    "            mask = df_imputed['aqi_category'].isnull()\n",
    "            df_imputed.loc[mask, 'aqi_category'] = df_imputed.loc[mask, 'air_quality_aqi'].apply(\n",
    "                lambda x: 'Good' if x <= 50 else 'Moderate' if x <= 100 else 'Unhealthy'\n",
    "            )\n",
    "            after_aqi_cat = df_imputed['aqi_category'].isnull().sum()\n",
    "            filled = before_aqi_cat - after_aqi_cat\n",
    "            print(f\"   aqi_category: Re-derived {filled} categories from AQI values\")\n",
    "    \n",
    "    # 10.3 Validation after imputation\n",
    "    print(\"\\\\n✅ POST-IMPUTATION VALIDATION\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    remaining_missing = df_imputed.isnull().sum().sum()\n",
    "    print(f\"Remaining missing values: {remaining_missing}\")\n",
    "    \n",
    "    if remaining_missing > 0:\n",
    "        remaining_cols = df_imputed.columns[df_imputed.isnull().any()].tolist()\n",
    "        print(f\"Columns with remaining missing values: {remaining_cols}\")\n",
    "        \n",
    "        # Show options for remaining missing values\n",
    "        print(\"\\\\n💡 Options for remaining missing values:\")\n",
    "        print(\"   - Drop rows with missing values\")\n",
    "        print(\"   - Drop columns with too many missing values\")\n",
    "        print(\"   - Use advanced imputation methods (KNN, etc.)\")\n",
    "    else:\n",
    "        print(\"🎉 All missing values have been handled!\")\n",
    "    \n",
    "    # Compare before and after\n",
    "    print(\"\\\\n📊 BEFORE vs AFTER Comparison:\")\n",
    "    comparison = pd.DataFrame({\n",
    "        'Column': sensor_cols,\n",
    "        'Original_Missing': [df_clean[col].isnull().sum() for col in sensor_cols],\n",
    "        'After_Imputation': [df_imputed[col].isnull().sum() for col in sensor_cols]\n",
    "    })\n",
    "    print(comparison)\n",
    "    \n",
    "    # Save the imputed dataset for later use\n",
    "    df_final = df_imputed.copy()\n",
    "    print(\"\\\\n✅ Final dataset prepared for export!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Tidak dapat melakukan missing value treatment - dataset kosong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f27d1",
   "metadata": {},
   "source": [
    "## 11. Export Processed Data (LOAD)\n",
    "Terakhir, mari export data yang sudah diproses ke berbagai format untuk penggunaan selanjutnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.1 Export to various formats\n",
    "if 'df_final' in locals() and not df_final.empty:\n",
    "    print(\"💾 EXPORTING PROCESSED DATA\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    import os\n",
    "    output_dir = 'data/output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp for file naming\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 11.1.1 Export to CSV\n",
    "    print(\"📄 Exporting to CSV...\")\n",
    "    csv_filename = f\"{output_dir}/processed_sensor_data_{timestamp}.csv\"\n",
    "    df_final.to_csv(csv_filename, index=False)\n",
    "    print(f\"   ✅ Saved: {csv_filename}\")\n",
    "    \n",
    "    # 11.1.2 Export to Excel with multiple sheets\n",
    "    print(\"\\\\n📊 Exporting to Excel (multiple sheets)...\")\n",
    "    excel_filename = f\"{output_dir}/sensor_analysis_{timestamp}.xlsx\"\n",
    "    \n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "        # Main processed data\n",
    "        df_final.to_excel(writer, sheet_name='Processed_Data', index=False)\n",
    "        \n",
    "        # Summary statistics\n",
    "        summary_stats = df_final.describe()\n",
    "        summary_stats.to_excel(writer, sheet_name='Summary_Statistics')\n",
    "        \n",
    "        # Location summary (if available)\n",
    "        if 'location' in df_final.columns:\n",
    "            location_summary = df_final.groupby('location').agg({\n",
    "                'temperature_celsius': ['mean', 'min', 'max'],\n",
    "                'humidity_percent': ['mean', 'min', 'max'],\n",
    "                'air_quality_aqi': 'mean'\n",
    "            }).round(2)\n",
    "            location_summary.to_excel(writer, sheet_name='Location_Summary')\n",
    "        \n",
    "        # Daily aggregation (if timestamp available)\n",
    "        if 'timestamp' in df_final.columns:\n",
    "            daily_agg = df_final.groupby(df_final['timestamp'].dt.date).agg({\n",
    "                'temperature_celsius': 'mean',\n",
    "                'humidity_percent': 'mean',\n",
    "                'air_quality_aqi': 'mean'\n",
    "            }).round(2)\n",
    "            daily_agg.to_excel(writer, sheet_name='Daily_Averages')\n",
    "    \n",
    "    print(f\"   ✅ Saved: {excel_filename}\")\n",
    "    \n",
    "    # 11.1.3 Export to JSON\n",
    "    print(\"\\\\n🔗 Exporting to JSON...\")\n",
    "    json_filename = f\"{output_dir}/processed_sensor_data_{timestamp}.json\"\n",
    "    \n",
    "    # Convert datetime columns to string for JSON serialization\n",
    "    df_json = df_final.copy()\n",
    "    for col in df_json.columns:\n",
    "        if df_json[col].dtype == 'datetime64[ns]':\n",
    "            df_json[col] = df_json[col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        elif pd.api.types.is_categorical_dtype(df_json[col]):\n",
    "            df_json[col] = df_json[col].astype(str)\n",
    "    \n",
    "    df_json.to_json(json_filename, orient='records', indent=2)\n",
    "    print(f\"   ✅ Saved: {json_filename}\")\n",
    "    \n",
    "    # 11.1.4 Export to Parquet (efficient for large datasets)\n",
    "    print(\"\\\\n🗜️ Exporting to Parquet...\")\n",
    "    try:\n",
    "        parquet_filename = f\"{output_dir}/processed_sensor_data_{timestamp}.parquet\"\n",
    "        df_final.to_parquet(parquet_filename, index=False)\n",
    "        print(f\"   ✅ Saved: {parquet_filename}\")\n",
    "    except ImportError:\n",
    "        print(\"   ⚠️ Parquet export requires 'pyarrow' or 'fastparquet'. Install with: pip install pyarrow\")\n",
    "    \n",
    "    # 11.2 Export aggregated summaries\n",
    "    print(\"\\\\n📋 EXPORTING SUMMARY REPORTS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # 11.2.1 Data Quality Report\n",
    "    quality_report = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Total Records',\n",
    "            'Date Range',\n",
    "            'Unique Sensors', \n",
    "            'Unique Locations',\n",
    "            'Missing Values',\n",
    "            'Duplicate Records (removed)',\n",
    "            'Processing Timestamp'\n",
    "        ],\n",
    "        'Value': [\n",
    "            len(df_final),\n",
    "            f\"{df_final['timestamp'].min()} to {df_final['timestamp'].max()}\" if 'timestamp' in df_final.columns else 'N/A',\n",
    "            df_final['sensor_id'].nunique() if 'sensor_id' in df_final.columns else 'N/A',\n",
    "            df_final['location'].nunique() if 'location' in df_final.columns else 'N/A',\n",
    "            df_final.isnull().sum().sum(),\n",
    "            f\"{len(df_main) - len(df_final)} removed\" if 'df_main' in locals() else 'N/A',\n",
    "            datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    quality_filename = f\"{output_dir}/data_quality_report_{timestamp}.csv\"\n",
    "    quality_report.to_csv(quality_filename, index=False)\n",
    "    print(f\"📊 Quality Report: {quality_filename}\")\n",
    "    \n",
    "    # 11.2.2 Column Metadata\n",
    "    metadata = pd.DataFrame({\n",
    "        'Column': df_final.columns,\n",
    "        'Data_Type': df_final.dtypes.astype(str),\n",
    "        'Non_Null_Count': df_final.count(),\n",
    "        'Null_Count': df_final.isnull().sum(),\n",
    "        'Unique_Values': [df_final[col].nunique() for col in df_final.columns],\n",
    "        'Sample_Values': [str(df_final[col].dropna().iloc[:3].tolist()) if len(df_final[col].dropna()) > 0 else 'N/A' for col in df_final.columns]\n",
    "    })\n",
    "    \n",
    "    metadata_filename = f\"{output_dir}/column_metadata_{timestamp}.csv\"\n",
    "    metadata.to_csv(metadata_filename, index=False)\n",
    "    print(f\"📝 Column Metadata: {metadata_filename}\")\n",
    "    \n",
    "    # 11.3 Export configuration for reproducibility\n",
    "    print(\"\\\\n⚙️ EXPORTING PROCESSING CONFIG\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    processing_config = {\n",
    "        'processing_timestamp': datetime.now().isoformat(),\n",
    "        'original_data_shape': list(df_main.shape) if 'df_main' in locals() else [0, 0],\n",
    "        'final_data_shape': list(df_final.shape),\n",
    "        'processing_steps': [\n",
    "            'Data extraction from multiple formats',\n",
    "            'Duplicate removal',\n",
    "            'Column standardization',\n",
    "            'Time series processing',\n",
    "            'Feature engineering',\n",
    "            'Missing value imputation',\n",
    "            'Data validation'\n",
    "        ],\n",
    "        'feature_engineering': [\n",
    "            'Temperature conversions (F, K)',\n",
    "            'Heat index calculation',\n",
    "            'Air quality categorization',\n",
    "            'Comfort index creation',\n",
    "            'Time-based features',\n",
    "            'Rolling averages'\n",
    "        ],\n",
    "        'data_quality': {\n",
    "            'duplicates_removed': len(df_main) - len(df_final) if 'df_main' in locals() else 0,\n",
    "            'missing_values_imputed': True,\n",
    "            'outliers_detected': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_filename = f\"{output_dir}/processing_config_{timestamp}.json\"\n",
    "    with open(config_filename, 'w') as f:\n",
    "        json.dump(processing_config, f, indent=2)\n",
    "    print(f\"⚙️ Processing Config: {config_filename}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\\\n🎉 ETL PIPELINE COMPLETED!\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"📊 Processed {len(df_final):,} sensor records\")\n",
    "    print(f\"🏷️ Created {len(df_final.columns)} features\")\n",
    "    print(f\"📁 Generated {5} output files\")\n",
    "    print(f\"📅 Processing completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    print(\"\\\\n📁 Output files created:\")\n",
    "    output_files = [\n",
    "        csv_filename,\n",
    "        excel_filename,\n",
    "        json_filename,\n",
    "        quality_filename,\n",
    "        metadata_filename,\n",
    "        config_filename\n",
    "    ]\n",
    "    \n",
    "    for file in output_files:\n",
    "        if os.path.exists(file):\n",
    "            size_mb = os.path.getsize(file) / (1024*1024)\n",
    "            print(f\"   📄 {os.path.basename(file)} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No processed data available for export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9dfe4",
   "metadata": {},
   "source": [
    "## 🎯 Kesimpulan dan Next Steps\n",
    "\n",
    "Selamat! Anda telah menyelesaikan tutorial hands-on ETL untuk data sensor menggunakan Pandas. \n",
    "\n",
    "### ✅ Yang Telah Dipelajari:\n",
    "1. **Extract**: Membaca data dari berbagai format (CSV, Excel, JSON)\n",
    "2. **Transform**: \n",
    "   - Data cleaning dan preprocessing\n",
    "   - Time series handling\n",
    "   - Feature engineering\n",
    "   - Aggregation dan grouping\n",
    "   - Missing value treatment\n",
    "3. **Load**: Export ke berbagai format output\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "1. **Advanced Analytics**: Gunakan data yang sudah diproses untuk:\n",
    "   - Time series forecasting\n",
    "   - Anomaly detection\n",
    "   - Statistical analysis\n",
    "   \n",
    "2. **Automation**: Buat ETL pipeline yang berjalan otomatis\n",
    "3. **Visualization**: Buat dashboard untuk monitoring sensor\n",
    "4. **Machine Learning**: Gunakan data untuk predictive modeling\n",
    "\n",
    "### 📚 Resources Tambahan:\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Time Series Analysis with Pandas](https://pandas.pydata.org/docs/user_guide/timeseries.html)\n",
    "- [Data Cleaning Best Practices](https://pandas.pydata.org/docs/user_guide/missing_data.html)\n",
    "\n",
    "Terima kasih telah mengikuti tutorial ini! 🙏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46788f1",
   "metadata": {},
   "source": [
    "### 💭 A. Pertanyaan Essay\n",
    "\n",
    "**1. Jelaskan perbedaan antara Forward Fill dan Interpolation dalam menangani missing values pada data time series. Kapan sebaiknya menggunakan masing-masing metode?**\n",
    "\n",
    "**2. Mengapa normalisasi data penting dalam preprocessing data sensor? Berikan contoh kasus dimana normalisasi diperlukan.**\n",
    "\n",
    "**3. Sebutkan dan jelaskan 3 metode deteksi outlier yang bisa digunakan untuk data sensor. Apa kelebihan dan kekurangan masing-masing?**\n",
    "\n",
    "**4. Bagaimana cara memvalidasi kualitas hasil ETL pipeline? Sebutkan minimal 5 metrik yang bisa digunakan.**\n",
    "\n",
    "**5. Jelaskan pentingnya feature engineering dalam preprocessing data sensor. Berikan 3 contoh feature yang bisa dibuat dari data suhu dan kelembaban.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287cc2b",
   "metadata": {},
   "source": [
    "### 🏋️ B. Latihan Praktis (Hands-On Challenges)\n",
    "\n",
    "Sekarang saatnya menguji kemampuan coding Anda! Kerjakan latihan-latihan berikut menggunakan data yang sudah diproses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed9013",
   "metadata": {},
   "source": [
    "#### 💪 Challenge 1: Data Quality Analysis\n",
    "**Tugas**: Buat function untuk menghitung comprehensive data quality score yang mencakup:\n",
    "- Completeness (% data yang tidak missing)\n",
    "- Validity (% data dalam range yang valid)  \n",
    "- Consistency (% data yang konsisten formatnya)\n",
    "- Uniqueness (% data yang tidak duplikat)\n",
    "\n",
    "**Target**: Function harus return dictionary dengan score 0-100 untuk setiap metrik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f00c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Implementasi Data Quality Score Function\n",
    "# TODO: Lengkapi function berikut\n",
    "\n",
    "def calculate_data_quality_comprehensive(df):\n",
    "    \"\"\"\n",
    "    Hitung comprehensive data quality score\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame yang akan dianalisis\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary dengan quality scores\n",
    "    \"\"\"\n",
    "    \n",
    "    quality_scores = {}\n",
    "    \n",
    "    # 1. COMPLETENESS SCORE\n",
    "    # TODO: Hitung persentase data yang tidak missing\n",
    "    total_cells = df.size\n",
    "    missing_cells = df.isnull().sum().sum()\n",
    "    quality_scores['completeness'] = 0  # REPLACE THIS LINE\n",
    "    \n",
    "    # 2. VALIDITY SCORE  \n",
    "    # TODO: Hitung persentase data sensor dalam range valid\n",
    "    # Gunakan range: temp (-50,60), humidity (0,100), pressure (900,1100), aqi (0,500)\n",
    "    valid_ranges = {\n",
    "        'temperature_celsius': (-50, 60),\n",
    "        'humidity_percent': (0, 100),\n",
    "        'pressure_hpa': (900, 1100),\n",
    "        'air_quality_aqi': (0, 500)\n",
    "    }\n",
    "    \n",
    "    validity_scores = []\n",
    "    # TODO: Implementasi validity check untuk setiap kolom\n",
    "    \n",
    "    quality_scores['validity'] = 0  # REPLACE THIS LINE\n",
    "    \n",
    "    # 3. UNIQUENESS SCORE\n",
    "    # TODO: Hitung persentase data yang tidak duplikat\n",
    "    quality_scores['uniqueness'] = 0  # REPLACE THIS LINE\n",
    "    \n",
    "    # 4. CONSISTENCY SCORE\n",
    "    # TODO: Check format consistency (contoh: semua location dalam Title Case)\n",
    "    quality_scores['consistency'] = 0  # REPLACE THIS LINE\n",
    "    \n",
    "    # 5. OVERALL SCORE (weighted average)\n",
    "    weights = {'completeness': 0.3, 'validity': 0.3, 'uniqueness': 0.2, 'consistency': 0.2}\n",
    "    quality_scores['overall'] = sum(quality_scores[key] * weights[key] for key in weights.keys())\n",
    "    \n",
    "    return quality_scores\n",
    "\n",
    "# Test your function (uncomment when ready)\n",
    "# if 'df_final' in locals():\n",
    "#     scores = calculate_data_quality_comprehensive(df_final)\n",
    "#     print(\"Data Quality Scores:\")\n",
    "#     for metric, score in scores.items():\n",
    "#         print(f\"  {metric.title()}: {score:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
