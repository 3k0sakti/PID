{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51aa8b2d",
   "metadata": {},
   "source": [
    "# Hands-on: Load Data ke Data Warehouse\n",
    "\n",
    "## üèóÔ∏è **Tutorial Lengkap ETL Pipeline untuk Data Warehouse**\n",
    "\n",
    "**Oleh**: Data Engineering Team  \n",
    "**Tanggal**: Oktober 2025  \n",
    "**Durasi**: 2-3 jam  \n",
    "\n",
    "---\n",
    "\n",
    "## üìñ **Overview**\n",
    "\n",
    "Tutorial hands-on ini akan mengajarkan Anda cara mengimplementasikan **Extract, Transform, Load (ETL)** pipeline untuk memuat data sensor IoT ke dalam data warehouse. Anda akan belajar:\n",
    "\n",
    "1. **Extract**: Membaca data dari file CSV\n",
    "2. **Transform**: Membersihkan dan memproses data\n",
    "3. **Load**: Memuat data ke warehouse dengan struktur star schema\n",
    "4. **Validate**: Memverifikasi kualitas dan integritas data\n",
    "5. **Analyze**: Menjalankan query analytics\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Learning Objectives**\n",
    "\n",
    "Setelah menyelesaikan tutorial ini, Anda akan dapat:\n",
    "\n",
    "‚úÖ Memahami konsep data warehouse dan star schema  \n",
    "‚úÖ Mengimplementasikan ETL pipeline dengan Python  \n",
    "‚úÖ Melakukan data profiling dan quality assessment  \n",
    "‚úÖ Merancang dan membuat dimension dan fact tables  \n",
    "‚úÖ Mengoptimalkan performance untuk data loading  \n",
    "‚úÖ Membuat analytics queries untuk business insights  \n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Dataset**\n",
    "\n",
    "**Data Sensor IoT** dengan informasi:\n",
    "- **File**: `processed_sensor_data_20250930_092513.csv`\n",
    "- **Records**: ~28,816 rows\n",
    "- **Periode**: Januari 2024\n",
    "- **Sensors**: 10 sensors di 5 lokasi\n",
    "- **Metrics**: Temperature, Humidity, Pressure, Air Quality\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è **Prerequisites**\n",
    "\n",
    "- Python 3.8+\n",
    "- Basic understanding of SQL\n",
    "- Familiarity dengan pandas\n",
    "- VS Code dengan Jupyter extension\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e4fdee",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Import Required Libraries\n",
    "\n",
    "Mari kita mulai dengan mengimpor semua library yang diperlukan untuk ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for ETL pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "\n",
    "# Auto-install SQLAlchemy if not available\n",
    "try:\n",
    "    from sqlalchemy import create_engine, text\n",
    "    print(\"‚úÖ SQLAlchemy available\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing SQLAlchemy...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sqlalchemy>=2.0.0\"])\n",
    "    from sqlalchemy import create_engine, text\n",
    "    print(\"‚úÖ SQLAlchemy installed and imported\")\n",
    "\n",
    "# Import our custom modules with fallback\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.append('./src')\n",
    "    from src.config import *\n",
    "    from src.data_loader import DataLoader\n",
    "    from src.warehouse_manager import WarehouseManager\n",
    "    print(\"‚úÖ Custom modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Custom modules not found: {e}\")\n",
    "    print(\"üîß Creating inline configuration...\")\n",
    "    \n",
    "    # Inline configuration as fallback\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "    WAREHOUSE_DIR = PROJECT_ROOT / \"warehouse\"\n",
    "    CSV_FILE = \"processed_sensor_data_20250930_092513.csv\"\n",
    "    CSV_PATH = DATA_DIR / CSV_FILE\n",
    "    DATABASE_URL = f\"sqlite:///{WAREHOUSE_DIR}/sensor_warehouse.db\"\n",
    "    DATABASE_FILE = WAREHOUSE_DIR / \"sensor_warehouse.db\"\n",
    "    BATCH_SIZE = 1000\n",
    "    \n",
    "    # Ensure directories exist\n",
    "    DATA_DIR.mkdir(exist_ok=True)\n",
    "    WAREHOUSE_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"‚úÖ Inline configuration created\")\n",
    "\n",
    "print(f\"üìÇ Project root: {PROJECT_ROOT if 'PROJECT_ROOT' in locals() else Path.cwd()}\")\n",
    "print(f\"üìä CSV file: {CSV_PATH if 'CSV_PATH' in locals() else 'Not configured'}\")\n",
    "print(f\"üóÑÔ∏è Database: {DATABASE_FILE if 'DATABASE_FILE' in locals() else 'Not configured'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d51a7",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Connect to Data Sources\n",
    "\n",
    "Sebelum memulai ETL, mari kita periksa data source yang tersedia dan inisialisasi DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataLoader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Check if source file exists\n",
    "if CSV_PATH.exists():\n",
    "    print(f\"‚úÖ Source file found: {CSV_PATH}\")\n",
    "    print(f\"üìè File size: {CSV_PATH.stat().st_size / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Source file not found: {CSV_PATH}\")\n",
    "    print(\"Please ensure the CSV file is in the correct location\")\n",
    "\n",
    "# Show current working directory and file structure\n",
    "print(f\"\\nüìÅ Current working directory: {Path.cwd()}\")\n",
    "print(\"\\nüìÇ Project structure:\")\n",
    "for item in PROJECT_ROOT.iterdir():\n",
    "    if item.is_file():\n",
    "        print(f\"  üìÑ {item.name}\")\n",
    "    elif item.is_dir():\n",
    "        print(f\"  üìÅ {item.name}/\")\n",
    "\n",
    "# Initialize warehouse manager (will create db if not exists)\n",
    "warehouse = WarehouseManager()\n",
    "print(f\"\\nüóÑÔ∏è Warehouse manager initialized\")\n",
    "print(f\"Database URL: {DATABASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd02661",
   "metadata": {},
   "source": [
    "## üîß **Auto ETL Pipeline Check**\n",
    "\n",
    "Sebelum melanjutkan, mari kita pastikan database sudah siap. Jika belum, kita akan menjalankan ETL pipeline secara otomatis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto ETL Pipeline Execution with Fallback\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"üîç CHECKING DATABASE STATUS...\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Initialize components if not already done\n",
    "try:\n",
    "    if 'data_loader' not in locals():\n",
    "        if 'DataLoader' in locals():\n",
    "            data_loader = DataLoader()\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è DataLoader not available, creating simple inline ETL...\")\n",
    "            data_loader = None\n",
    "    \n",
    "    if 'warehouse' not in locals():\n",
    "        if 'WarehouseManager' in locals():\n",
    "            warehouse = WarehouseManager()\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è WarehouseManager not available, using direct SQLite...\")\n",
    "            warehouse = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Component initialization warning: {e}\")\n",
    "\n",
    "# Check if database exists and has data\n",
    "database_exists = DATABASE_FILE.exists() if 'DATABASE_FILE' in locals() else False\n",
    "database_has_data = False\n",
    "\n",
    "if database_exists:\n",
    "    try:\n",
    "        import sqlite3\n",
    "        conn = sqlite3.connect(str(DATABASE_FILE))\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='fact_sensor_readings'\")\n",
    "        table_exists = cursor.fetchone() is not None\n",
    "        \n",
    "        if table_exists:\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM fact_sensor_readings\")\n",
    "            record_count = cursor.fetchone()[0]\n",
    "            conn.close()\n",
    "            \n",
    "            if record_count > 0:\n",
    "                database_has_data = True\n",
    "                print(f\"‚úÖ Database found with {record_count:,} records\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Database exists but fact table is empty\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Database exists but fact table not found\")\n",
    "            conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Database check error: {e}\")\n",
    "\n",
    "if not database_exists or not database_has_data:\n",
    "    print(\"\\nüöÄ RUNNING AUTO ETL PIPELINE...\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Check if CSV file exists\n",
    "    if not CSV_PATH.exists():\n",
    "        print(f\"‚ùå Source CSV file not found: {CSV_PATH}\")\n",
    "        print(\"üí° Please ensure the CSV file is in the correct location:\")\n",
    "        print(f\"   Expected: {CSV_PATH}\")\n",
    "        print(\"\\nüìÅ Available files in project:\")\n",
    "        for item in PROJECT_ROOT.iterdir():\n",
    "            if item.is_file() and item.suffix == '.csv':\n",
    "                print(f\"   üìÑ {item.name}\")\n",
    "        \n",
    "        # Try to find CSV in root directory\n",
    "        csv_files = list(PROJECT_ROOT.glob(\"*.csv\"))\n",
    "        if csv_files:\n",
    "            print(f\"\\nüîß Found CSV in root: {csv_files[0].name}\")\n",
    "            print(\"Moving to data directory...\")\n",
    "            CSV_PATH = DATA_DIR / csv_files[0].name\n",
    "            csv_files[0].rename(CSV_PATH)\n",
    "            print(f\"‚úÖ Moved to: {CSV_PATH}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No CSV file found. Please place sensor data CSV in {DATA_DIR}\")\n",
    "    \n",
    "    try:\n",
    "        if data_loader and warehouse:\n",
    "            # Use full ETL classes\n",
    "            print(\"üîß Using full ETL pipeline...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            print(\"üì• Step 1/3: Extracting data...\")\n",
    "            raw_data = data_loader.extract_data()\n",
    "            extraction_time = time.time() - start_time\n",
    "            \n",
    "            print(\"üîÑ Step 2/3: Transforming data...\")\n",
    "            transform_start = time.time()\n",
    "            clean_data = data_loader.transform_data()\n",
    "            transformation_time = time.time() - transform_start\n",
    "            \n",
    "            print(\"üì§ Step 3/3: Loading to warehouse...\")\n",
    "            load_start = time.time()\n",
    "            data_loader.load_to_warehouse()\n",
    "            loading_time = time.time() - load_start\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"\\n‚úÖ ETL PIPELINE COMPLETED!\")\n",
    "            print(f\"‚è±Ô∏è Total time: {total_time:.2f} seconds\")\n",
    "            \n",
    "        else:\n",
    "            # Fallback: Simple inline ETL\n",
    "            print(\"üîß Using simplified inline ETL...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Simple extraction\n",
    "            print(\"üì• Loading CSV data...\")\n",
    "            raw_data = pd.read_csv(CSV_PATH)\n",
    "            print(f\"   Loaded {len(raw_data):,} rows\")\n",
    "            \n",
    "            # Simple transformation\n",
    "            print(\"üîÑ Basic data cleaning...\")\n",
    "            clean_data = raw_data.drop_duplicates()\n",
    "            clean_data['timestamp'] = pd.to_datetime(clean_data['timestamp'])\n",
    "            print(f\"   Cleaned to {len(clean_data):,} rows\")\n",
    "            \n",
    "            # Simple loading\n",
    "            print(\"üì§ Loading to SQLite...\")\n",
    "            WAREHOUSE_DIR.mkdir(exist_ok=True)\n",
    "            conn = sqlite3.connect(str(DATABASE_FILE))\n",
    "            \n",
    "            # Create simple fact table\n",
    "            clean_data.to_sql('fact_sensor_readings', conn, if_exists='replace', index=False)\n",
    "            \n",
    "            conn.close()\n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"‚úÖ Simple ETL completed in {total_time:.2f} seconds\")\n",
    "        \n",
    "        # Verify the results\n",
    "        conn = sqlite3.connect(str(DATABASE_FILE))\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM fact_sensor_readings\")\n",
    "        final_count = cursor.fetchone()[0]\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"üìä Final verification: {final_count:,} records in warehouse\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ETL Pipeline failed: {e}\")\n",
    "        print(\"üí° Alternative options:\")\n",
    "        print(\"   1. Run ./run_etl.sh manually in terminal\")\n",
    "        print(\"   2. Check if all required files are in place\")\n",
    "        print(\"   3. Install missing dependencies: pip install -r requirements.txt\")\n",
    "        raise\n",
    "        \n",
    "else:\n",
    "    print(\"‚úÖ Database is ready to use!\")\n",
    "\n",
    "print(f\"\\nüéØ READY FOR ANALYSIS!\")\n",
    "print(\"You can now proceed with the rest of the notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07813e1",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Extract Data from Sources\n",
    "\n",
    "Mari kita ekstrak data dari CSV file dan lakukan exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from CSV\n",
    "print(\"üîÑ Extracting data from source...\")\n",
    "start_time = time.time()\n",
    "\n",
    "raw_data = data_loader.extract_data()\n",
    "extraction_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Extraction completed in {extraction_time:.2f} seconds\")\n",
    "print(f\"üìä Dataset shape: {raw_data.shape}\")\n",
    "print(f\"üíæ Memory usage: {raw_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nüìã Dataset Info:\")\n",
    "print(f\"‚Ä¢ Rows: {raw_data.shape[0]:,}\")\n",
    "print(f\"‚Ä¢ Columns: {raw_data.shape[1]}\")\n",
    "print(f\"‚Ä¢ Data types: {raw_data.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nüëÄ First 5 rows:\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c79525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data profiling for better understanding\n",
    "print(\"üîç Performing data profiling...\")\n",
    "profile = data_loader.data_profiling()\n",
    "\n",
    "print(f\"\\nüìä Data Profile Summary:\")\n",
    "print(f\"‚Ä¢ Shape: {profile['shape']}\")\n",
    "print(f\"‚Ä¢ Missing values: {sum(profile['missing_values'].values())} total\")\n",
    "print(f\"‚Ä¢ Duplicate rows: {profile['duplicates']}\")\n",
    "print(f\"‚Ä¢ Memory usage: {profile['memory_usage']}\")\n",
    "\n",
    "# Show columns and their types\n",
    "print(f\"\\nüìã Column Information:\")\n",
    "for col, dtype in profile['dtypes'].items():\n",
    "    missing = profile['missing_values'][col]\n",
    "    missing_pct = (missing / profile['shape'][0]) * 100\n",
    "    print(f\"  {col:<35} | {str(dtype):<10} | Missing: {missing:>5} ({missing_pct:>5.1f}%)\")\n",
    "\n",
    "# Show unique values for categorical columns\n",
    "print(f\"\\nüè∑Ô∏è Categorical Columns Summary:\")\n",
    "for col, stats in profile['categorical_stats'].items():\n",
    "    print(f\"  {col}: {stats['unique_count']} unique values\")\n",
    "    if stats['unique_count'] <= 10:\n",
    "        print(f\"    Values: {list(stats['top_values'].keys())}\")\n",
    "    else:\n",
    "        print(f\"    Top 3: {list(stats['top_values'].keys())[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d11523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('üìä Sensor Data Distribution Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Temperature distribution\n",
    "axes[0,0].hist(raw_data['temperature_celsius'], bins=50, alpha=0.7, color='red')\n",
    "axes[0,0].set_title('üå°Ô∏è Temperature Distribution')\n",
    "axes[0,0].set_xlabel('Temperature (¬∞C)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "\n",
    "# Humidity distribution\n",
    "axes[0,1].hist(raw_data['humidity_percent'], bins=50, alpha=0.7, color='blue')\n",
    "axes[0,1].set_title('üíß Humidity Distribution')\n",
    "axes[0,1].set_xlabel('Humidity (%)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# Air Quality distribution\n",
    "axes[1,0].hist(raw_data['air_quality_aqi'], bins=50, alpha=0.7, color='green')\n",
    "axes[1,0].set_title('üè≠ Air Quality (AQI) Distribution')\n",
    "axes[1,0].set_xlabel('AQI')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Pressure distribution\n",
    "axes[1,1].hist(raw_data['pressure_hpa'], bins=50, alpha=0.7, color='purple')\n",
    "axes[1,1].set_title('üå™Ô∏è Pressure Distribution')\n",
    "axes[1,1].set_xlabel('Pressure (hPa)')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show sensor and location counts\n",
    "print(\"\\nüè∑Ô∏è Sensor and Location Analysis:\")\n",
    "print(f\"Unique sensors: {raw_data['sensor_id'].nunique()}\")\n",
    "print(f\"Sensor IDs: {sorted(raw_data['sensor_id'].unique())}\")\n",
    "print(f\"\\nUnique locations: {raw_data['location'].nunique()}\")\n",
    "print(f\"Locations: {sorted(raw_data['location'].unique())}\")\n",
    "\n",
    "# Time range analysis\n",
    "raw_data['timestamp'] = pd.to_datetime(raw_data['timestamp'])\n",
    "print(f\"\\nüìÖ Time Range:\")\n",
    "print(f\"From: {raw_data['timestamp'].min()}\")\n",
    "print(f\"To: {raw_data['timestamp'].max()}\")\n",
    "print(f\"Duration: {raw_data['timestamp'].max() - raw_data['timestamp'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41a8c0",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Transform and Clean Data\n",
    "\n",
    "Sekarang kita akan melakukan data transformation, validasi kualitas, dan cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db455de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data quality\n",
    "print(\"üîç Validating data quality...\")\n",
    "validation_results = data_loader.validate_data()\n",
    "\n",
    "print(f\"\\nüìä Data Validation Results:\")\n",
    "print(f\"‚Ä¢ Total records: {validation_results['total_records']:,}\")\n",
    "print(f\"‚Ä¢ Validation status: {'‚úÖ PASSED' if validation_results['is_valid'] else '‚ùå FAILED'}\")\n",
    "\n",
    "if validation_results['errors']:\n",
    "    print(f\"\\n‚ùå Errors found:\")\n",
    "    for error in validation_results['errors']:\n",
    "        print(f\"  ‚Ä¢ {error}\")\n",
    "\n",
    "if validation_results['warnings']:\n",
    "    print(f\"\\n‚ö†Ô∏è Warnings:\")\n",
    "    for warning in validation_results['warnings']:\n",
    "        print(f\"  ‚Ä¢ {warning}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No data quality warnings found!\")\n",
    "\n",
    "# Show data ranges for key metrics\n",
    "key_metrics = ['temperature_celsius', 'humidity_percent', 'pressure_hpa', 'air_quality_aqi']\n",
    "print(f\"\\nüìà Data Ranges:\")\n",
    "for metric in key_metrics:\n",
    "    if metric in raw_data.columns:\n",
    "        min_val = raw_data[metric].min()\n",
    "        max_val = raw_data[metric].max()\n",
    "        mean_val = raw_data[metric].mean()\n",
    "        print(f\"  {metric:<25}: {min_val:>8.2f} to {max_val:>8.2f} (avg: {mean_val:>8.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and clean the data\n",
    "print(\"üîÑ Transforming and cleaning data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "clean_data = data_loader.transform_data()\n",
    "transformation_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Transformation completed in {transformation_time:.2f} seconds\")\n",
    "print(f\"üìä Original dataset: {raw_data.shape[0]:,} rows\")\n",
    "print(f\"üìä Clean dataset: {clean_data.shape[0]:,} rows\")\n",
    "print(f\"üìä Rows removed: {raw_data.shape[0] - clean_data.shape[0]:,}\")\n",
    "\n",
    "# Compare before and after\n",
    "comparison_data = []\n",
    "for metric in key_metrics:\n",
    "    if metric in raw_data.columns:\n",
    "        original_nulls = raw_data[metric].isnull().sum()\n",
    "        clean_nulls = clean_data[metric].isnull().sum()\n",
    "        comparison_data.append({\n",
    "            'Metric': metric,\n",
    "            'Original Nulls': original_nulls,\n",
    "            'Clean Nulls': clean_nulls,\n",
    "            'Original Range': f\"{raw_data[metric].min():.2f} - {raw_data[metric].max():.2f}\",\n",
    "            'Clean Range': f\"{clean_data[metric].min():.2f} - {clean_data[metric].max():.2f}\"\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(f\"\\nüìã Before vs After Cleaning:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Quality score\n",
    "quality_score = data_loader._calculate_quality_score()\n",
    "print(f\"\\nüèÜ Data Quality Score: {quality_score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd008a39",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Connect to Data Warehouse\n",
    "\n",
    "Mari kita setup data warehouse dengan star schema design - dimension tables dan fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a55f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to warehouse and create schema\n",
    "print(\"üóÑÔ∏è Connecting to data warehouse...\")\n",
    "warehouse.connect()\n",
    "\n",
    "print(\"üèóÔ∏è Creating warehouse schema (Star Schema)...\")\n",
    "warehouse.create_tables()\n",
    "\n",
    "print(\"‚úÖ Warehouse schema created successfully!\")\n",
    "\n",
    "# Show the star schema design\n",
    "print(f\"\\n‚≠ê Star Schema Design:\")\n",
    "print(f\"üìä Fact Table: {FACT_TABLE}\")\n",
    "print(f\"üìã Dimension Tables:\")\n",
    "print(f\"  ‚Ä¢ {DIM_SENSOR_TABLE}\")\n",
    "print(f\"  ‚Ä¢ {DIM_LOCATION_TABLE}\")\n",
    "print(f\"  ‚Ä¢ {DIM_TIME_TABLE}\")\n",
    "\n",
    "# Prepare dimension data\n",
    "print(f\"\\nüîÑ Preparing dimension data...\")\n",
    "dimensions = data_loader.prepare_dimension_data()\n",
    "\n",
    "print(f\"üìä Dimension Data Summary:\")\n",
    "for dim_name, dim_data in dimensions.items():\n",
    "    print(f\"  ‚Ä¢ {dim_name}: {len(dim_data)} records\")\n",
    "    print(f\"    Columns: {list(dim_data.columns)}\")\n",
    "\n",
    "# Show sample dimension data\n",
    "print(f\"\\nüëÄ Sample Dimension Data:\")\n",
    "print(f\"\\nüîß Sensors Dimension:\")\n",
    "print(dimensions['sensors'].head())\n",
    "\n",
    "print(f\"\\nüìç Locations Dimension:\")\n",
    "print(dimensions['locations'].head())\n",
    "\n",
    "print(f\"\\nüìÖ Time Dimension (first 5 records):\")\n",
    "print(dimensions['time'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed2878",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Load Data to Warehouse Tables\n",
    "\n",
    "Sekarang kita akan load data ke warehouse menggunakan batch processing untuk optimasi performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to warehouse (if not already done)\n",
    "print(\"üöÄ WAREHOUSE LOADING STATUS CHECK...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if we need to load data\n",
    "conn = sqlite3.connect(str(DATABASE_FILE))\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Check if tables exist and have data\n",
    "try:\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM fact_sensor_readings\")\n",
    "    existing_records = cursor.fetchone()[0]\n",
    "    \n",
    "    if existing_records > 0:\n",
    "        print(f\"‚úÖ Data already loaded: {existing_records:,} records found\")\n",
    "        loading_time = 0\n",
    "    else:\n",
    "        print(\"üîÑ Loading data to warehouse...\")\n",
    "        # Data loading was already done in the auto ETL check above\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM fact_sensor_readings\")\n",
    "        existing_records = cursor.fetchone()[0]\n",
    "        loading_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Data loading verified: {existing_records:,} records\")\n",
    "        \n",
    "except sqlite3.OperationalError:\n",
    "    print(\"‚ùå Tables not found. Please ensure ETL pipeline ran successfully.\")\n",
    "    \n",
    "conn.close()\n",
    "\n",
    "# Get loading summary with fallback\n",
    "try:\n",
    "    if 'data_loader' in locals() and data_loader:\n",
    "        load_summary = data_loader.get_load_summary()\n",
    "    else:\n",
    "        # Create simple summary\n",
    "        load_summary = {\n",
    "            \"clean_data_rows\": existing_records,\n",
    "            \"data_quality_score\": 100.0,\n",
    "            \"load_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "except:\n",
    "    load_summary = {\n",
    "        \"clean_data_rows\": existing_records,\n",
    "        \"data_quality_score\": 100.0,\n",
    "        \"load_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "print(f\"\\nüìä Loading Summary:\")\n",
    "for key, value in load_summary.items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "# Check warehouse table information with fallback\n",
    "print(f\"\\nüóÑÔ∏è Warehouse Tables Information:\")\n",
    "conn = sqlite3.connect(str(DATABASE_FILE))\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Get list of tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = cursor.fetchall()\n",
    "    \n",
    "    total_records = 0\n",
    "    for table_tuple in tables:\n",
    "        table_name = table_tuple[0]\n",
    "        if not table_name.startswith('sqlite_'):  # Skip system tables\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "            record_count = cursor.fetchone()[0]\n",
    "            total_records += record_count\n",
    "            print(f\"  üìã {table_name}: {record_count:,} records\")\n",
    "    \n",
    "    print(f\"\\nüìä Total records in warehouse: {total_records:,}\")\n",
    "    if loading_time > 0:\n",
    "        print(f\"üìà Loading rate: {total_records/loading_time:.0f} records/second\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking tables: {e}\")\n",
    "    \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330d877",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Validate Data Loading\n",
    "\n",
    "Mari kita verifikasi bahwa data sudah ter-load dengan benar dan jalankan beberapa quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54426549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation queries with fallback\n",
    "print(\"üîç Running data validation queries...\")\n",
    "\n",
    "# Create simple warehouse query function if not available\n",
    "def execute_query_simple(query):\n",
    "    \"\"\"Simple query executor as fallback\"\"\"\n",
    "    conn = sqlite3.connect(str(DATABASE_FILE))\n",
    "    try:\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Query error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Use warehouse manager if available, otherwise use simple function\n",
    "if 'warehouse' in locals() and warehouse:\n",
    "    query_executor = warehouse.execute_query\n",
    "else:\n",
    "    query_executor = execute_query_simple\n",
    "\n",
    "# 1. Row count validation\n",
    "query_row_counts = \"\"\"\n",
    "SELECT \n",
    "    'fact_sensor_readings' as table_name, COUNT(*) as record_count \n",
    "FROM fact_sensor_readings\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    row_counts = query_executor(query_row_counts)\n",
    "    print(\"üìä Row Count Validation:\")\n",
    "    print(row_counts.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Row count query failed: {e}\")\n",
    "\n",
    "# 2. Data completeness check\n",
    "query_completeness = \"\"\"\n",
    "SELECT \n",
    "    'Temperature' as metric,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(temperature_celsius) as non_null_records,\n",
    "    ROUND(COUNT(temperature_celsius) * 100.0 / COUNT(*), 2) as completeness_percent\n",
    "FROM fact_sensor_readings\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Humidity' as metric,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(humidity_percent) as non_null_records,\n",
    "    ROUND(COUNT(humidity_percent) * 100.0 / COUNT(*), 2) as completeness_percent\n",
    "FROM fact_sensor_readings\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Air Quality' as metric,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(air_quality_aqi) as non_null_records,\n",
    "    ROUND(COUNT(air_quality_aqi) * 100.0 / COUNT(*), 2) as completeness_percent\n",
    "FROM fact_sensor_readings\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    completeness = query_executor(query_completeness)\n",
    "    print(f\"\\nüìà Data Completeness Validation:\")\n",
    "    print(completeness.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Completeness query failed: {e}\")\n",
    "\n",
    "# 3. Data range validation\n",
    "query_ranges = \"\"\"\n",
    "SELECT \n",
    "    'Temperature (¬∞C)' as metric,\n",
    "    ROUND(MIN(temperature_celsius), 2) as min_value,\n",
    "    ROUND(MAX(temperature_celsius), 2) as max_value,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_value\n",
    "FROM fact_sensor_readings\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Humidity (%)' as metric,\n",
    "    ROUND(MIN(humidity_percent), 2) as min_value,\n",
    "    ROUND(MAX(humidity_percent), 2) as max_value,\n",
    "    ROUND(AVG(humidity_percent), 2) as avg_value\n",
    "FROM fact_sensor_readings\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Air Quality (AQI)' as metric,\n",
    "    ROUND(MIN(air_quality_aqi), 2) as min_value,\n",
    "    ROUND(MAX(air_quality_aqi), 2) as max_value,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_value\n",
    "FROM fact_sensor_readings\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    ranges = query_executor(query_ranges)\n",
    "    print(f\"\\nüìä Data Range Validation:\")\n",
    "    print(ranges.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Range query failed: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data validation completed!\")\n",
    "print(f\"üí° Database is ready for analytics queries and visualizations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646e0b0",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Monitor Loading Performance\n",
    "\n",
    "Mari kita analisis performa ETL pipeline dan buat metrics untuk monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6979ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Analytics Queries with fallback\n",
    "print(\"üìä Running Advanced Analytics Queries...\")\n",
    "\n",
    "# Create simple warehouse query function if not available\n",
    "def execute_analytics_query(query):\n",
    "    \"\"\"Analytics query executor with error handling\"\"\"\n",
    "    conn = sqlite3.connect(str(DATABASE_FILE))\n",
    "    try:\n",
    "        result = pd.read_sql_query(query, conn)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Query error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Use warehouse manager if available, otherwise use simple function\n",
    "if 'warehouse' in locals() and warehouse:\n",
    "    analytics_executor = warehouse.execute_query\n",
    "else:\n",
    "    analytics_executor = execute_analytics_query\n",
    "\n",
    "# Query 1: Daily sensor summary\n",
    "query_daily_summary = \"\"\"\n",
    "SELECT \n",
    "    d.date_key,\n",
    "    d.date_full,\n",
    "    d.day_name,\n",
    "    COUNT(*) as total_readings,\n",
    "    ROUND(AVG(f.temperature_celsius), 2) as avg_temperature,\n",
    "    ROUND(AVG(f.humidity_percent), 2) as avg_humidity,\n",
    "    ROUND(AVG(f.air_quality_aqi), 2) as avg_air_quality\n",
    "FROM fact_sensor_readings f\n",
    "JOIN dim_date d ON f.date_key = d.date_key\n",
    "GROUP BY d.date_key, d.date_full, d.day_name\n",
    "ORDER BY d.date_key\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    daily_summary = analytics_executor(query_daily_summary)\n",
    "    if not daily_summary.empty:\n",
    "        print(\"üìÖ Daily Sensor Summary (First 10 days):\")\n",
    "        print(daily_summary.to_string(index=False))\n",
    "    else:\n",
    "        print(\"‚ùå No daily summary data found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Daily summary query failed: {e}\")\n",
    "\n",
    "# Query 2: Sensor performance by location\n",
    "query_sensor_performance = \"\"\"\n",
    "SELECT \n",
    "    l.location_name,\n",
    "    l.location_type,\n",
    "    s.sensor_type,\n",
    "    COUNT(*) as reading_count,\n",
    "    ROUND(AVG(f.temperature_celsius), 2) as avg_temperature,\n",
    "    ROUND(AVG(f.humidity_percent), 2) as avg_humidity,\n",
    "    ROUND(AVG(f.air_quality_aqi), 2) as avg_air_quality\n",
    "FROM fact_sensor_readings f\n",
    "JOIN dim_sensor s ON f.sensor_key = s.sensor_key\n",
    "JOIN dim_location l ON f.location_key = l.location_key\n",
    "GROUP BY l.location_name, l.location_type, s.sensor_type\n",
    "ORDER BY reading_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    sensor_performance = analytics_executor(query_sensor_performance)\n",
    "    if not sensor_performance.empty:\n",
    "        print(f\"\\nüå°Ô∏è Sensor Performance by Location (Top 10):\")\n",
    "        print(sensor_performance.to_string(index=False))\n",
    "    else:\n",
    "        print(\"‚ùå No sensor performance data found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Sensor performance query failed: {e}\")\n",
    "\n",
    "# Query 3: Hourly patterns\n",
    "query_hourly_patterns = \"\"\"\n",
    "SELECT \n",
    "    t.hour_24,\n",
    "    t.hour_12_period,\n",
    "    COUNT(*) as reading_count,\n",
    "    ROUND(AVG(f.temperature_celsius), 2) as avg_temperature,\n",
    "    ROUND(AVG(f.humidity_percent), 2) as avg_humidity,\n",
    "    ROUND(AVG(f.air_quality_aqi), 2) as avg_air_quality\n",
    "FROM fact_sensor_readings f\n",
    "JOIN dim_time t ON f.time_key = t.time_key\n",
    "GROUP BY t.hour_24, t.hour_12_period\n",
    "ORDER BY t.hour_24\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    hourly_patterns = analytics_executor(query_hourly_patterns)\n",
    "    if not hourly_patterns.empty:\n",
    "        print(f\"\\n‚è∞ Hourly Reading Patterns (24-hour cycle):\")\n",
    "        print(hourly_patterns.to_string(index=False))\n",
    "    else:\n",
    "        print(\"‚ùå No hourly pattern data found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Hourly patterns query failed: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Advanced analytics queries completed!\")\n",
    "print(f\"üí° These queries show how to leverage the star schema for complex analytics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fad423",
   "metadata": {},
   "source": [
    "## üéØ **Business Analytics Queries**\n",
    "\n",
    "Sekarang kita akan menjalankan beberapa business analytics queries untuk mendapatkan insights dari data warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219087c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Analytics Query 1: Location Performance Analysis\n",
    "query_location_analysis = \"\"\"\n",
    "SELECT \n",
    "    l.location_name,\n",
    "    COUNT(DISTINCT s.sensor_id) as active_sensors,\n",
    "    COUNT(*) as total_readings,\n",
    "    ROUND(AVG(f.temperature_celsius), 2) as avg_temperature,\n",
    "    ROUND(AVG(f.humidity_percent), 2) as avg_humidity,\n",
    "    ROUND(AVG(f.air_quality_aqi), 2) as avg_aqi,\n",
    "    MIN(f.timestamp) as first_reading,\n",
    "    MAX(f.timestamp) as last_reading\n",
    "FROM fact_sensor_readings f\n",
    "JOIN dim_sensors s ON f.sensor_key = s.sensor_key\n",
    "JOIN dim_locations l ON f.location_key = l.location_key\n",
    "GROUP BY l.location_name\n",
    "ORDER BY avg_aqi DESC;\n",
    "\"\"\"\n",
    "\n",
    "location_analysis = warehouse.execute_query(query_location_analysis)\n",
    "print(\"üìç Location Performance Analysis:\")\n",
    "print(location_analysis.to_string(index=False))\n",
    "\n",
    "# Visualize location analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üìç Location-Based Analytics Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Average temperature by location\n",
    "axes[0,0].bar(location_analysis['location_name'], location_analysis['avg_temperature'], color='red', alpha=0.7)\n",
    "axes[0,0].set_title('üå°Ô∏è Average Temperature by Location')\n",
    "axes[0,0].set_ylabel('Temperature (¬∞C)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average AQI by location\n",
    "axes[0,1].bar(location_analysis['location_name'], location_analysis['avg_aqi'], color='green', alpha=0.7)\n",
    "axes[0,1].set_title('üè≠ Average Air Quality by Location')\n",
    "axes[0,1].set_ylabel('AQI')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Total readings by location\n",
    "axes[1,0].bar(location_analysis['location_name'], location_analysis['total_readings'], color='blue', alpha=0.7)\n",
    "axes[1,0].set_title('üìä Total Readings by Location')\n",
    "axes[1,0].set_ylabel('Reading Count')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average humidity by location\n",
    "axes[1,1].bar(location_analysis['location_name'], location_analysis['avg_humidity'], color='cyan', alpha=0.7)\n",
    "axes[1,1].set_title('üíß Average Humidity by Location')\n",
    "axes[1,1].set_ylabel('Humidity (%)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Analytics Query 2: Temporal Patterns\n",
    "query_temporal_patterns = \"\"\"\n",
    "SELECT \n",
    "    t.time_period,\n",
    "    COUNT(*) as reading_count,\n",
    "    ROUND(AVG(f.temperature_celsius), 2) as avg_temperature,\n",
    "    ROUND(AVG(f.humidity_percent), 2) as avg_humidity,\n",
    "    ROUND(AVG(f.air_quality_aqi), 2) as avg_aqi\n",
    "FROM fact_sensor_readings f\n",
    "JOIN dim_time t ON f.time_key = t.time_key\n",
    "GROUP BY t.time_period\n",
    "ORDER BY \n",
    "    CASE t.time_period \n",
    "        WHEN 'Night' THEN 1\n",
    "        WHEN 'Morning' THEN 2\n",
    "        WHEN 'Afternoon' THEN 3\n",
    "        WHEN 'Evening' THEN 4\n",
    "        ELSE 5\n",
    "    END;\n",
    "\"\"\"\n",
    "\n",
    "temporal_patterns = warehouse.execute_query(query_temporal_patterns)\n",
    "print(f\"\\n‚è∞ Temporal Patterns Analysis:\")\n",
    "print(temporal_patterns.to_string(index=False))\n",
    "\n",
    "# Interactive plotly visualization for temporal patterns\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Temperature Patterns', 'Air Quality Patterns', \n",
    "                    'Humidity Patterns', 'Reading Distribution'),\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Temperature by time period\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=temporal_patterns['time_period'], y=temporal_patterns['avg_temperature'],\n",
    "               mode='lines+markers', name='Temperature', line=dict(color='red')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# AQI by time period\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=temporal_patterns['time_period'], y=temporal_patterns['avg_aqi'],\n",
    "               mode='lines+markers', name='AQI', line=dict(color='green')),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Humidity by time period\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=temporal_patterns['time_period'], y=temporal_patterns['avg_humidity'],\n",
    "               mode='lines+markers', name='Humidity', line=dict(color='blue')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Reading count by time period\n",
    "fig.add_trace(\n",
    "    go.Bar(x=temporal_patterns['time_period'], y=temporal_patterns['reading_count'],\n",
    "           name='Reading Count', marker=dict(color='purple')),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, showlegend=False, title_text=\"‚è∞ Temporal Analytics Dashboard\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Analytics Query 3: Sensor Performance & Reliability\n",
    "query_sensor_performance = \"\"\"\n",
    "SELECT \n",
    "    s.sensor_id,\n",
    "    l.location_name,\n",
    "    s.status,\n",
    "    COUNT(*) as reading_count,\n",
    "    ROUND(AVG(f.temperature_celsius), 2) as avg_temperature,\n",
    "    ROUND(AVG(f.humidity_percent), 2) as avg_humidity,\n",
    "    ROUND(AVG(f.air_quality_aqi), 2) as avg_aqi,\n",
    "    MIN(f.timestamp) as first_reading,\n",
    "    MAX(f.timestamp) as last_reading\n",
    "FROM fact_sensor_readings f\n",
    "JOIN dim_sensors s ON f.sensor_key = s.sensor_key\n",
    "JOIN dim_locations l ON f.location_key = l.location_key\n",
    "GROUP BY s.sensor_id, l.location_name, s.status\n",
    "ORDER BY reading_count DESC;\n",
    "\"\"\"\n",
    "\n",
    "sensor_performance = warehouse.execute_query(query_sensor_performance)\n",
    "print(f\"\\nüîß Sensor Performance & Reliability Analysis:\")\n",
    "print(sensor_performance.to_string(index=False))\n",
    "\n",
    "# Business Analytics Query 4: Air Quality Categories Distribution\n",
    "query_aqi_distribution = \"\"\"\n",
    "SELECT \n",
    "    aqi_category,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM fact_sensor_readings), 2) as percentage\n",
    "FROM fact_sensor_readings\n",
    "WHERE aqi_category IS NOT NULL\n",
    "GROUP BY aqi_category\n",
    "ORDER BY count DESC;\n",
    "\"\"\"\n",
    "\n",
    "aqi_distribution = warehouse.execute_query(query_aqi_distribution)\n",
    "print(f\"\\nüè≠ Air Quality Categories Distribution:\")\n",
    "print(aqi_distribution.to_string(index=False))\n",
    "\n",
    "# Create AQI distribution pie chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# AQI Category Distribution\n",
    "ax1.pie(aqi_distribution['percentage'], labels=aqi_distribution['aqi_category'], \n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('üè≠ Air Quality Categories Distribution')\n",
    "\n",
    "# Sensor reading count by sensor\n",
    "sensor_counts = sensor_performance.groupby('sensor_id')['reading_count'].sum().sort_values(ascending=False)\n",
    "ax2.bar(range(len(sensor_counts)), sensor_counts.values, color='skyblue')\n",
    "ax2.set_title('üìä Readings per Sensor')\n",
    "ax2.set_xlabel('Sensor ID')\n",
    "ax2.set_ylabel('Reading Count')\n",
    "ax2.set_xticks(range(len(sensor_counts)))\n",
    "ax2.set_xticklabels(sensor_counts.index, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd3e42",
   "metadata": {},
   "source": [
    "## üéì **Key Learnings & Best Practices**\n",
    "\n",
    "Selamat! Anda telah berhasil menyelesaikan hands-on ETL pipeline untuk data warehouse. Mari kita review apa yang telah dipelajari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb3958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and cleanup\n",
    "print(\"üéâ ETL Pipeline Completed Successfully!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Final summary\n",
    "final_summary = {\n",
    "    'üìä Total Records Loaded': f\"{total_records:,}\",\n",
    "    'üóÑÔ∏è Tables Created': \"4 (1 Fact + 3 Dimensions)\",\n",
    "    '‚è±Ô∏è Total Processing Time': f\"{extraction_time + transformation_time + loading_time:.2f} seconds\",\n",
    "    'üèÜ Data Quality Score': f\"{quality_score}%\",\n",
    "    'üìà Processing Rate': f\"{records_per_second:.0f} records/second\",\n",
    "    'üíæ Database Size': f\"{DATABASE_FILE.stat().st_size / 1024**2:.2f} MB\" if DATABASE_FILE.exists() else \"N/A\"\n",
    "}\n",
    "\n",
    "print(\"üìã Final ETL Summary:\")\n",
    "for key, value in final_summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n‚úÖ What we accomplished:\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ Extracted {raw_data.shape[0]:,} records from CSV\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ Cleaned and validated data (removed {raw_data.shape[0] - clean_data.shape[0]:,} duplicates)\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ Designed star schema with fact and dimension tables\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ Loaded data with {BATCH_SIZE:,} records per batch\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ Created indexes for query optimization\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ Ran analytics queries and generated insights\")\n",
    "print(f\"  ‚Ä¢ ‚úÖ Monitored performance and quality metrics\")\n",
    "\n",
    "print(f\"\\nüéØ Key Insights Discovered:\")\n",
    "best_location = location_analysis.loc[location_analysis['avg_aqi'].idxmin(), 'location_name']\n",
    "worst_location = location_analysis.loc[location_analysis['avg_aqi'].idxmax(), 'location_name']\n",
    "best_time = temporal_patterns.loc[temporal_patterns['avg_aqi'].idxmin(), 'time_period']\n",
    "most_active_sensor = sensor_performance.loc[sensor_performance['reading_count'].idxmax(), 'sensor_id']\n",
    "\n",
    "print(f\"  ‚Ä¢ üèÜ Best air quality location: {best_location}\")\n",
    "print(f\"  ‚Ä¢ ‚ö†Ô∏è Highest AQI location: {worst_location}\")\n",
    "print(f\"  ‚Ä¢ ‚è∞ Best air quality time: {best_time}\")\n",
    "print(f\"  ‚Ä¢ üîß Most active sensor: {most_active_sensor}\")\n",
    "\n",
    "print(f\"\\nüí° Next Steps & Recommendations:\")\n",
    "print(f\"  ‚Ä¢ üîÑ Set up automated ETL pipeline with scheduler (Apache Airflow)\")\n",
    "print(f\"  ‚Ä¢ üìä Create real-time dashboards (Grafana, Tableau)\")\n",
    "print(f\"  ‚Ä¢ üîç Implement data quality monitoring and alerting\")\n",
    "print(f\"  ‚Ä¢ üìà Add more advanced analytics (forecasting, anomaly detection)\")\n",
    "print(f\"  ‚Ä¢ üèóÔ∏è Scale to cloud data warehouse (BigQuery, Snowflake, Redshift)\")\n",
    "print(f\"  ‚Ä¢ üîí Implement data governance and security measures\")\n",
    "\n",
    "# Close warehouse connection\n",
    "warehouse.close()\n",
    "print(f\"\\nüîå Database connection closed.\")\n",
    "print(f\"üéä Tutorial completed successfully! Well done!\")\n",
    "\n",
    "# Show final file structure\n",
    "print(f\"\\nüìÅ Final Project Structure:\")\n",
    "for item in sorted(PROJECT_ROOT.rglob(\"*\")):\n",
    "    if item.is_file() and not item.name.startswith('.'):\n",
    "        rel_path = item.relative_to(PROJECT_ROOT)\n",
    "        indent = \"  \" * len(rel_path.parts)\n",
    "        print(f\"{indent}üìÑ {item.name}\")\n",
    "    elif item.is_dir() and not item.name.startswith('.'):\n",
    "        rel_path = item.relative_to(PROJECT_ROOT)\n",
    "        if len(rel_path.parts) <= 2:  # Only show first 2 levels\n",
    "            indent = \"  \" * len(rel_path.parts)\n",
    "            print(f\"{indent}üìÅ {item.name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b7106",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ **Congratulations!**\n",
    "\n",
    "**Anda telah berhasil menyelesaikan hands-on ETL pipeline untuk data warehouse!**\n",
    "\n",
    "### üìö **Yang Telah Dipelajari:**\n",
    "\n",
    "1. **üìä Data Warehouse Concepts**\n",
    "   - Star schema design (fact tables + dimension tables)\n",
    "   - ETL (Extract, Transform, Load) pipeline\n",
    "   - Data quality assessment dan validation\n",
    "\n",
    "2. **üõ†Ô∏è Technical Skills**\n",
    "   - Python untuk data processing (pandas, SQLAlchemy)\n",
    "   - Database design dan optimization (indexes, foreign keys)\n",
    "   - Batch processing untuk large datasets\n",
    "   - Performance monitoring dan metrics\n",
    "\n",
    "3. **üìà Analytics & Insights**\n",
    "   - Business intelligence queries\n",
    "   - Data visualization dengan matplotlib dan plotly\n",
    "   - KPI tracking dan dashboard creation\n",
    "\n",
    "### üéØ **Best Practices yang Diimplementasikan:**\n",
    "\n",
    "‚úÖ **Data Quality**: Validation, cleaning, outlier handling  \n",
    "‚úÖ **Performance**: Batch processing, indexing, connection pooling  \n",
    "‚úÖ **Scalability**: Modular code, configurable parameters  \n",
    "‚úÖ **Monitoring**: Logging, metrics, error handling  \n",
    "‚úÖ **Documentation**: Clear code comments dan docstrings  \n",
    "\n",
    "### üöÄ **Next Level Challenges:**\n",
    "\n",
    "- **üîÑ Real-time ETL**: Kafka + Spark Streaming\n",
    "- **‚òÅÔ∏è Cloud Platforms**: AWS/GCP/Azure data warehouses\n",
    "- **ü§ñ ML Integration**: Feature stores, model serving\n",
    "- **üìä Advanced Analytics**: Time series forecasting, anomaly detection\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Data Engineering! üéâ**\n",
    "\n",
    "*Untuk pertanyaan atau diskusi lebih lanjut, silakan hubungi tim Data Engineering.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6db5a8",
   "metadata": {},
   "source": [
    "## üîç **Advanced Database Queries & Verification**\n",
    "\n",
    "Mari kita jalankan beberapa query advanced untuk memverifikasi dan mengeksplorasi data di warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278640c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Query Verification\n",
    "print(\"üîç COMPREHENSIVE DATABASE VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if database exists and connect\n",
    "if DATABASE_FILE.exists():\n",
    "    print(f\"‚úÖ Database found: {DATABASE_FILE}\")\n",
    "    print(f\"üìè Database size: {DATABASE_FILE.stat().st_size / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Database not found. Please run ETL pipeline first.\")\n",
    "\n",
    "# Connect to database for queries\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(str(DATABASE_FILE))\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Show database schema\n",
    "print(f\"\\nüìã DATABASE SCHEMA:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "tables = ['dim_sensors', 'dim_locations', 'dim_time', 'fact_sensor_readings']\n",
    "for table in tables:\n",
    "    cursor.execute(f'PRAGMA table_info({table})')\n",
    "    schema = cursor.fetchall()\n",
    "    print(f\"\\nüè∑Ô∏è {table.upper()}:\")\n",
    "    for col in schema[:5]:  # Show first 5 columns\n",
    "        print(f\"  ‚Ä¢ {col[1]:<25} {col[2]:<15}\")\n",
    "    if len(schema) > 5:\n",
    "        print(f\"  ... and {len(schema)-5} more columns\")\n",
    "\n",
    "# Check row counts\n",
    "print(f\"\\nüìä TABLE ROW COUNTS:\")\n",
    "print(\"-\" * 30)\n",
    "total_records = 0\n",
    "for table in tables:\n",
    "    cursor.execute(f'SELECT COUNT(*) FROM {table}')\n",
    "    count = cursor.fetchone()[0]\n",
    "    total_records += count\n",
    "    print(f\"  üìã {table:<25}: {count:>8,} records\")\n",
    "\n",
    "print(f\"  {'TOTAL':<25}: {total_records:>8,} records\")\n",
    "\n",
    "# Sample data preview\n",
    "print(f\"\\nüëÄ SAMPLE DATA PREVIEW:\")\n",
    "print(\"-\" * 30)\n",
    "cursor.execute('SELECT * FROM fact_sensor_readings LIMIT 3')\n",
    "sample_data = cursor.fetchall()\n",
    "cursor.execute('PRAGMA table_info(fact_sensor_readings)')\n",
    "columns = [col[1] for col in cursor.fetchall()]\n",
    "\n",
    "print(f\"Columns (first 8): {columns[:8]}\")\n",
    "for i, row in enumerate(sample_data, 1):\n",
    "    print(f\"Row {i}: {row[:8]}\")\n",
    "\n",
    "conn.close()\n",
    "print(f\"\\n‚úÖ Database verification completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Intelligence Queries\n",
    "print(\"üìä BUSINESS INTELLIGENCE QUERIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Query 1: Location Performance Analysis\n",
    "print(\"\\nüìç 1. LOCATION PERFORMANCE ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "query_locations = \"\"\"\n",
    "SELECT \n",
    "    location_key,\n",
    "    COUNT(*) as total_readings,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_temperature,\n",
    "    ROUND(AVG(humidity_percent), 2) as avg_humidity,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_aqi,\n",
    "    ROUND(MIN(temperature_celsius), 2) as min_temp,\n",
    "    ROUND(MAX(temperature_celsius), 2) as max_temp\n",
    "FROM fact_sensor_readings\n",
    "GROUP BY location_key\n",
    "ORDER BY avg_aqi DESC\n",
    "\"\"\"\n",
    "\n",
    "df_locations = warehouse.execute_query(query_locations)\n",
    "print(df_locations.to_string(index=False))\n",
    "\n",
    "# Query 2: Temporal Patterns\n",
    "print(\"\\n\\n‚è∞ 2. HOURLY TEMPERATURE & AQI PATTERNS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "query_hourly = \"\"\"\n",
    "SELECT \n",
    "    CAST(strftime('%H', timestamp) as INTEGER) as hour,\n",
    "    COUNT(*) as reading_count,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_temperature,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_aqi,\n",
    "    ROUND(MIN(temperature_celsius), 2) as min_temp,\n",
    "    ROUND(MAX(temperature_celsius), 2) as max_temp\n",
    "FROM fact_sensor_readings\n",
    "GROUP BY CAST(strftime('%H', timestamp) as INTEGER)\n",
    "ORDER BY hour\n",
    "\"\"\"\n",
    "\n",
    "df_hourly = warehouse.execute_query(query_hourly)\n",
    "print(df_hourly.head(12).to_string(index=False))\n",
    "\n",
    "# Query 3: Air Quality Distribution\n",
    "print(\"\\n\\nüè≠ 3. AIR QUALITY CATEGORIES DISTRIBUTION\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "query_aqi = \"\"\"\n",
    "SELECT \n",
    "    aqi_category,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM fact_sensor_readings), 2) as percentage,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_aqi_value,\n",
    "    ROUND(MIN(air_quality_aqi), 2) as min_aqi,\n",
    "    ROUND(MAX(air_quality_aqi), 2) as max_aqi\n",
    "FROM fact_sensor_readings\n",
    "WHERE aqi_category IS NOT NULL\n",
    "GROUP BY aqi_category\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "df_aqi = warehouse.execute_query(query_aqi)\n",
    "print(df_aqi.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Trends Analysis\n",
    "print(\"\\nüìÖ 4. DAILY TRENDS ANALYSIS (First 10 days)\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "query_daily = \"\"\"\n",
    "SELECT \n",
    "    DATE(timestamp) as date,\n",
    "    strftime('%w', timestamp) as day_of_week,\n",
    "    CASE strftime('%w', timestamp)\n",
    "        WHEN '0' THEN 'Sunday'\n",
    "        WHEN '1' THEN 'Monday'\n",
    "        WHEN '2' THEN 'Tuesday'\n",
    "        WHEN '3' THEN 'Wednesday'\n",
    "        WHEN '4' THEN 'Thursday'\n",
    "        WHEN '5' THEN 'Friday'\n",
    "        WHEN '6' THEN 'Saturday'\n",
    "    END as day_name,\n",
    "    COUNT(*) as daily_readings,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_temp,\n",
    "    ROUND(AVG(humidity_percent), 2) as avg_humidity,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_aqi\n",
    "FROM fact_sensor_readings\n",
    "GROUP BY DATE(timestamp)\n",
    "ORDER BY DATE(timestamp)\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df_daily = warehouse.execute_query(query_daily)\n",
    "print(df_daily.to_string(index=False))\n",
    "\n",
    "# Data Quality Assessment\n",
    "print(\"\\n\\n‚úÖ 5. DATA QUALITY ASSESSMENT\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "query_quality = \"\"\"\n",
    "SELECT \n",
    "    'Total Records' as metric,\n",
    "    COUNT(*) as value,\n",
    "    '100.0%' as completeness\n",
    "FROM fact_sensor_readings\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Temperature' as metric,\n",
    "    COUNT(temperature_celsius) as value,\n",
    "    ROUND(COUNT(temperature_celsius) * 100.0 / COUNT(*), 1) || '%' as completeness\n",
    "FROM fact_sensor_readings\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Humidity' as metric,\n",
    "    COUNT(humidity_percent) as value,\n",
    "    ROUND(COUNT(humidity_percent) * 100.0 / COUNT(*), 1) || '%' as completeness\n",
    "FROM fact_sensor_readings\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Air Quality' as metric,\n",
    "    COUNT(air_quality_aqi) as value,\n",
    "    ROUND(COUNT(air_quality_aqi) * 100.0 / COUNT(*), 1) || '%' as completeness\n",
    "FROM fact_sensor_readings\n",
    "\"\"\"\n",
    "\n",
    "df_quality = warehouse.execute_query(query_quality)\n",
    "print(df_quality.to_string(index=False))\n",
    "\n",
    "# Statistical Summary\n",
    "print(\"\\n\\nüìà 6. STATISTICAL SUMMARY\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "query_stats = \"\"\"\n",
    "SELECT \n",
    "    'Temperature (¬∞C)' as metric,\n",
    "    ROUND(MIN(temperature_celsius), 2) as min_value,\n",
    "    ROUND(MAX(temperature_celsius), 2) as max_value,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_value,\n",
    "    COUNT(*) as count\n",
    "FROM fact_sensor_readings\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Humidity (%)' as metric,\n",
    "    ROUND(MIN(humidity_percent), 2) as min_value,\n",
    "    ROUND(MAX(humidity_percent), 2) as max_value,\n",
    "    ROUND(AVG(humidity_percent), 2) as avg_value,\n",
    "    COUNT(*) as count\n",
    "FROM fact_sensor_readings\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Air Quality (AQI)' as metric,\n",
    "    ROUND(MIN(air_quality_aqi), 2) as min_value,\n",
    "    ROUND(MAX(air_quality_aqi), 2) as max_value,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_value,\n",
    "    COUNT(*) as count\n",
    "FROM fact_sensor_readings\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Pressure (hPa)' as metric,\n",
    "    ROUND(MIN(pressure_hpa), 2) as min_value,\n",
    "    ROUND(MAX(pressure_hpa), 2) as max_value,\n",
    "    ROUND(AVG(pressure_hpa), 2) as avg_value,\n",
    "    COUNT(*) as count\n",
    "FROM fact_sensor_readings\n",
    "\"\"\"\n",
    "\n",
    "df_stats = warehouse.execute_query(query_stats)\n",
    "print(df_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf225aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Query Interface\n",
    "print(\"üîß INTERACTIVE QUERY INTERFACE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def run_custom_query(query_description, sql_query):\n",
    "    \"\"\"Helper function to run custom queries with error handling\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nüîç {query_description}\")\n",
    "        print(\"-\" * len(query_description))\n",
    "        result = warehouse.execute_query(sql_query)\n",
    "        print(result.to_string(index=False))\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error executing query: {e}\")\n",
    "        return None\n",
    "\n",
    "# Advanced Analytics Queries\n",
    "print(\"\\nüìä ADVANCED ANALYTICS QUERIES:\")\n",
    "\n",
    "# Query: Sensor Reliability (readings per sensor)\n",
    "sensor_reliability_query = \"\"\"\n",
    "SELECT \n",
    "    sensor_key,\n",
    "    COUNT(*) as total_readings,\n",
    "    COUNT(DISTINCT DATE(timestamp)) as active_days,\n",
    "    ROUND(COUNT(*) * 1.0 / COUNT(DISTINCT DATE(timestamp)), 2) as avg_readings_per_day,\n",
    "    MIN(timestamp) as first_reading,\n",
    "    MAX(timestamp) as last_reading\n",
    "FROM fact_sensor_readings\n",
    "GROUP BY sensor_key\n",
    "ORDER BY total_readings DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "run_custom_query(\"SENSOR RELIABILITY ANALYSIS\", sensor_reliability_query)\n",
    "\n",
    "# Query: Weather Patterns (correlation analysis)\n",
    "correlation_query = \"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN temperature_celsius < 20 THEN 'Cool (<20¬∞C)'\n",
    "        WHEN temperature_celsius BETWEEN 20 AND 30 THEN 'Moderate (20-30¬∞C)'\n",
    "        ELSE 'Warm (>30¬∞C)'\n",
    "    END as temp_category,\n",
    "    CASE \n",
    "        WHEN air_quality_aqi <= 50 THEN 'Good'\n",
    "        WHEN air_quality_aqi <= 100 THEN 'Moderate'\n",
    "        ELSE 'Unhealthy'\n",
    "    END as aqi_category,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(AVG(humidity_percent), 2) as avg_humidity,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_temp,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_aqi\n",
    "FROM fact_sensor_readings\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN temperature_celsius < 20 THEN 'Cool (<20¬∞C)'\n",
    "        WHEN temperature_celsius BETWEEN 20 AND 30 THEN 'Moderate (20-30¬∞C)'\n",
    "        ELSE 'Warm (>30¬∞C)'\n",
    "    END,\n",
    "    CASE \n",
    "        WHEN air_quality_aqi <= 50 THEN 'Good'\n",
    "        WHEN air_quality_aqi <= 100 THEN 'Moderate'\n",
    "        ELSE 'Unhealthy'\n",
    "    END\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "run_custom_query(\"TEMPERATURE vs AIR QUALITY CORRELATION\", correlation_query)\n",
    "\n",
    "# Query: Time-based anomalies\n",
    "anomaly_query = \"\"\"\n",
    "SELECT \n",
    "    DATE(timestamp) as date,\n",
    "    COUNT(*) as daily_readings,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_temp,\n",
    "    ROUND(MAX(temperature_celsius), 2) as max_temp,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_aqi,\n",
    "    ROUND(MAX(air_quality_aqi), 2) as max_aqi,\n",
    "    CASE \n",
    "        WHEN MAX(temperature_celsius) > 50 OR MAX(air_quality_aqi) > 150 THEN 'Potential Anomaly'\n",
    "        ELSE 'Normal'\n",
    "    END as status\n",
    "FROM fact_sensor_readings\n",
    "GROUP BY DATE(timestamp)\n",
    "HAVING MAX(temperature_celsius) > 45 OR MAX(air_quality_aqi) > 120\n",
    "ORDER BY DATE(timestamp)\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "run_custom_query(\"POTENTIAL ANOMALIES DETECTION\", anomaly_query)\n",
    "\n",
    "print(f\"\\nüí° QUERY TIPS:\")\n",
    "print(\"‚Ä¢ Use warehouse.execute_query(sql) untuk menjalankan query custom\")\n",
    "print(\"‚Ä¢ Gunakan LIMIT untuk membatasi hasil query besar\")\n",
    "print(\"‚Ä¢ Combine dengan visualizations untuk insights yang lebih baik\")\n",
    "print(\"‚Ä¢ Eksplorasi relationship antar dimensions dan metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Results Visualization\n",
    "print(\"üìä VISUALIZING QUERY RESULTS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Get data for visualizations\n",
    "location_data = warehouse.execute_query(\"\"\"\n",
    "SELECT \n",
    "    location_key,\n",
    "    COUNT(*) as total_readings,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_temperature,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_aqi\n",
    "FROM fact_sensor_readings\n",
    "GROUP BY location_key\n",
    "ORDER BY location_key\n",
    "\"\"\")\n",
    "\n",
    "hourly_data = warehouse.execute_query(\"\"\"\n",
    "SELECT \n",
    "    CAST(strftime('%H', timestamp) as INTEGER) as hour,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_temperature,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_aqi\n",
    "FROM fact_sensor_readings\n",
    "GROUP BY CAST(strftime('%H', timestamp) as INTEGER)\n",
    "ORDER BY hour\n",
    "\"\"\")\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('üìä Warehouse Data Analytics Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Location Temperature Comparison\n",
    "axes[0,0].bar(location_data['location_key'].astype(str), location_data['avg_temperature'], \n",
    "              color='red', alpha=0.7)\n",
    "axes[0,0].set_title('üå°Ô∏è Average Temperature by Location')\n",
    "axes[0,0].set_xlabel('Location Key')\n",
    "axes[0,0].set_ylabel('Temperature (¬∞C)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Location AQI Comparison\n",
    "axes[0,1].bar(location_data['location_key'].astype(str), location_data['avg_aqi'], \n",
    "              color='green', alpha=0.7)\n",
    "axes[0,1].set_title('üè≠ Average AQI by Location')\n",
    "axes[0,1].set_xlabel('Location Key')\n",
    "axes[0,1].set_ylabel('AQI')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Data Distribution per Location\n",
    "axes[0,2].pie(location_data['total_readings'], labels=location_data['location_key'], \n",
    "              autopct='%1.1f%%', startangle=90)\n",
    "axes[0,2].set_title('üìä Data Distribution by Location')\n",
    "\n",
    "# 4. Hourly Temperature Pattern\n",
    "axes[1,0].plot(hourly_data['hour'], hourly_data['avg_temperature'], \n",
    "               marker='o', linewidth=2, markersize=6, color='red')\n",
    "axes[1,0].set_title('‚è∞ Temperature Pattern by Hour')\n",
    "axes[1,0].set_xlabel('Hour of Day')\n",
    "axes[1,0].set_ylabel('Temperature (¬∞C)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "axes[1,0].set_xticks(range(0, 24, 2))\n",
    "\n",
    "# 5. Hourly AQI Pattern\n",
    "axes[1,1].plot(hourly_data['hour'], hourly_data['avg_aqi'], \n",
    "               marker='s', linewidth=2, markersize=6, color='green')\n",
    "axes[1,1].set_title('‚è∞ AQI Pattern by Hour')\n",
    "axes[1,1].set_xlabel('Hour of Day')\n",
    "axes[1,1].set_ylabel('AQI')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].set_xticks(range(0, 24, 2))\n",
    "\n",
    "# 6. Correlation Scatter Plot\n",
    "temp_aqi_data = warehouse.execute_query(\"\"\"\n",
    "SELECT temperature_celsius, air_quality_aqi \n",
    "FROM fact_sensor_readings \n",
    "WHERE temperature_celsius IS NOT NULL AND air_quality_aqi IS NOT NULL\n",
    "LIMIT 1000\n",
    "\"\"\")\n",
    "\n",
    "axes[1,2].scatter(temp_aqi_data['temperature_celsius'], temp_aqi_data['air_quality_aqi'], \n",
    "                  alpha=0.6, s=20)\n",
    "axes[1,2].set_title('üîó Temperature vs AQI Correlation')\n",
    "axes[1,2].set_xlabel('Temperature (¬∞C)')\n",
    "axes[1,2].set_ylabel('AQI')\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìà QUERY RESULTS SUMMARY:\")\n",
    "print(f\"‚Ä¢ Total locations analyzed: {len(location_data)}\")\n",
    "print(f\"‚Ä¢ Temperature range across locations: {location_data['avg_temperature'].min():.1f}¬∞C - {location_data['avg_temperature'].max():.1f}¬∞C\")\n",
    "print(f\"‚Ä¢ AQI range across locations: {location_data['avg_aqi'].min():.1f} - {location_data['avg_aqi'].max():.1f}\")\n",
    "print(f\"‚Ä¢ Hourly data points: {len(hourly_data)}\")\n",
    "print(f\"‚Ä¢ Peak temperature hour: {hourly_data.loc[hourly_data['avg_temperature'].idxmax(), 'hour']}:00\")\n",
    "print(f\"‚Ä¢ Best AQI hour: {hourly_data.loc[hourly_data['avg_aqi'].idxmin(), 'hour']}:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ecf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Query Playground\n",
    "print(\"üéÆ CUSTOM QUERY PLAYGROUND\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"üí° Sekarang Anda bisa mencoba query custom sendiri!\")\n",
    "print(\"Gunakan fungsi warehouse.execute_query(sql) untuk menjalankan query.\")\n",
    "print(\"\\nüìù Contoh query yang bisa dicoba:\")\n",
    "\n",
    "example_queries = {\n",
    "    \"1. Data Overview\": \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(DISTINCT sensor_key) as unique_sensors,\n",
    "    COUNT(DISTINCT location_key) as unique_locations,\n",
    "    MIN(timestamp) as start_date,\n",
    "    MAX(timestamp) as end_date\n",
    "FROM fact_sensor_readings\n",
    "\"\"\",\n",
    "    \n",
    "    \"2. Top 5 Warmest Days\": \"\"\"\n",
    "SELECT \n",
    "    DATE(timestamp) as date,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_temp,\n",
    "    ROUND(MAX(temperature_celsius), 2) as max_temp,\n",
    "    COUNT(*) as readings\n",
    "FROM fact_sensor_readings\n",
    "GROUP BY DATE(timestamp)\n",
    "ORDER BY avg_temp DESC\n",
    "LIMIT 5\n",
    "\"\"\",\n",
    "    \n",
    "    \"3. Weekend vs Weekday Analysis\": \"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN strftime('%w', timestamp) IN ('0', '6') THEN 'Weekend'\n",
    "        ELSE 'Weekday'\n",
    "    END as period,\n",
    "    COUNT(*) as readings,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_temp,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_aqi\n",
    "FROM fact_sensor_readings\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN strftime('%w', timestamp) IN ('0', '6') THEN 'Weekend'\n",
    "        ELSE 'Weekday'\n",
    "    END\n",
    "\"\"\",\n",
    "    \n",
    "    \"4. Monthly Trends\": \"\"\"\n",
    "SELECT \n",
    "    strftime('%Y-%m', timestamp) as month,\n",
    "    COUNT(*) as readings,\n",
    "    ROUND(AVG(temperature_celsius), 2) as avg_temp,\n",
    "    ROUND(AVG(humidity_percent), 2) as avg_humidity,\n",
    "    ROUND(AVG(air_quality_aqi), 2) as avg_aqi\n",
    "FROM fact_sensor_readings\n",
    "GROUP BY strftime('%Y-%m', timestamp)\n",
    "ORDER BY month\n",
    "\"\"\",\n",
    "    \n",
    "    \"5. Extreme Values\": \"\"\"\n",
    "SELECT \n",
    "    'Highest Temperature' as metric,\n",
    "    MAX(temperature_celsius) as value,\n",
    "    timestamp\n",
    "FROM fact_sensor_readings\n",
    "WHERE temperature_celsius = (SELECT MAX(temperature_celsius) FROM fact_sensor_readings)\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Lowest Temperature' as metric,\n",
    "    MIN(temperature_celsius) as value,\n",
    "    timestamp\n",
    "FROM fact_sensor_readings\n",
    "WHERE temperature_celsius = (SELECT MIN(temperature_celsius) FROM fact_sensor_readings)\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'Worst AQI' as metric,\n",
    "    MAX(air_quality_aqi) as value,\n",
    "    timestamp\n",
    "FROM fact_sensor_readings\n",
    "WHERE air_quality_aqi = (SELECT MAX(air_quality_aqi) FROM fact_sensor_readings)\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Display example queries\n",
    "for title, query in example_queries.items():\n",
    "    print(f\"\\nüîç {title}:\")\n",
    "    print(\"```sql\")\n",
    "    print(query.strip())\n",
    "    print(\"```\")\n",
    "\n",
    "print(f\"\\nüöÄ Cara menggunakan:\")\n",
    "print(\"# Copas query di atas ke cell baru dan jalankan seperti ini:\")\n",
    "print(\"# result = warehouse.execute_query('''QUERY_SQL_DISINI''')\")\n",
    "print(\"# print(result.to_string(index=False))\")\n",
    "\n",
    "print(f\"\\nüí≠ Tips untuk Query:\")\n",
    "print(\"‚Ä¢ Gunakan LIMIT untuk membatasi hasil\")\n",
    "print(\"‚Ä¢ Tambahkan ORDER BY untuk sorting\")\n",
    "print(\"‚Ä¢ Gunakan GROUP BY untuk aggregasi\")\n",
    "print(\"‚Ä¢ Kombinasikan dengan WHERE untuk filtering\")\n",
    "print(\"‚Ä¢ Test query sederhana dulu sebelum yang kompleks\")\n",
    "\n",
    "# Example: Run one of the queries\n",
    "print(f\"\\nüìä CONTOH: Menjalankan Query 'Data Overview'\")\n",
    "print(\"-\" * 45)\n",
    "try:\n",
    "    overview_result = warehouse.execute_query(example_queries[\"1. Data Overview\"])\n",
    "    print(overview_result.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ Selamat! Anda telah menguasai query database warehouse!\")\n",
    "print(\"üí° Silakan eksplorasi lebih lanjut dengan query custom Anda sendiri.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667bfc20",
   "metadata": {},
   "source": [
    "## üß† **Kuis: Test Pemahaman Data Warehouse**\n",
    "\n",
    "Setelah menyelesaikan hands-on tutorial ini, mari test pemahaman Anda tentang konsep data warehouse dan ETL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26200bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Quiz System\n",
    "import random\n",
    "\n",
    "class DataWarehouseQuiz:\n",
    "    def __init__(self):\n",
    "        self.score = 0\n",
    "        self.total_questions = 0\n",
    "        self.questions = self.load_questions()\n",
    "    \n",
    "    def load_questions(self):\n",
    "        return [\n",
    "            {\n",
    "                \"category\": \"Data Warehouse Concepts\",\n",
    "                \"question\": \"Apa yang dimaksud dengan Star Schema?\",\n",
    "                \"options\": [\n",
    "                    \"A. Database dengan tabel berbentuk bintang\",\n",
    "                    \"B. Schema dengan fact table di tengah dan dimension tables di sekitarnya\", \n",
    "                    \"C. Tabel dengan primary key berbentuk bintang\",\n",
    "                    \"D. Database yang hanya digunakan pada malam hari\"\n",
    "                ],\n",
    "                \"correct\": \"B\",\n",
    "                \"explanation\": \"Star Schema adalah desain database dimana fact table berada di tengah dan dikelilingi oleh dimension tables, membentuk pola seperti bintang.\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"ETL Process\",\n",
    "                \"question\": \"Urutan yang benar dalam proses ETL adalah:\",\n",
    "                \"options\": [\n",
    "                    \"A. Load ‚Üí Extract ‚Üí Transform\",\n",
    "                    \"B. Transform ‚Üí Extract ‚Üí Load\",\n",
    "                    \"C. Extract ‚Üí Transform ‚Üí Load\",\n",
    "                    \"D. Extract ‚Üí Load ‚Üí Transform\"\n",
    "                ],\n",
    "                \"correct\": \"C\",\n",
    "                \"explanation\": \"ETL adalah Extract (mengambil data), Transform (membersihkan/mengubah data), kemudian Load (memuat ke warehouse).\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"Database Design\",\n",
    "                \"question\": \"Dalam project ini, berapa jumlah total tabel yang dibuat di warehouse?\",\n",
    "                \"options\": [\n",
    "                    \"A. 2 tabel (1 fact + 1 dimension)\",\n",
    "                    \"B. 3 tabel (1 fact + 2 dimensions)\", \n",
    "                    \"C. 4 tabel (1 fact + 3 dimensions)\",\n",
    "                    \"D. 5 tabel (2 facts + 3 dimensions)\"\n",
    "                ],\n",
    "                \"correct\": \"C\",\n",
    "                \"explanation\": \"Dibuat 4 tabel: fact_sensor_readings, dim_sensors, dim_locations, dan dim_time.\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"Data Quality\",\n",
    "                \"question\": \"Berapa persen data completeness untuk temperature di warehouse kita?\",\n",
    "                \"options\": [\n",
    "                    \"A. 85%\",\n",
    "                    \"B. 95%\",\n",
    "                    \"C. 100%\",\n",
    "                    \"D. 75%\"\n",
    "                ],\n",
    "                \"correct\": \"C\",\n",
    "                \"explanation\": \"Semua field temperature memiliki completeness 100% setelah data cleaning process.\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"SQL Queries\",\n",
    "                \"question\": \"Untuk menganalisis trend per jam, fungsi SQL mana yang digunakan?\",\n",
    "                \"options\": [\n",
    "                    \"A. DATE(timestamp)\",\n",
    "                    \"B. strftime('%H', timestamp)\", \n",
    "                    \"C. HOUR(timestamp)\",\n",
    "                    \"D. TIME(timestamp)\"\n",
    "                ],\n",
    "                \"correct\": \"B\",\n",
    "                \"explanation\": \"Di SQLite, strftime('%H', timestamp) digunakan untuk extract jam dari timestamp.\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"Performance\",\n",
    "                \"question\": \"Kenapa kita menggunakan batch processing dalam loading data?\",\n",
    "                \"options\": [\n",
    "                    \"A. Untuk memperlambat proses loading\",\n",
    "                    \"B. Untuk menghemat memory dan meningkatkan performance\",\n",
    "                    \"C. Karena database tidak mendukung bulk insert\",\n",
    "                    \"D. Untuk membuat proses lebih rumit\"\n",
    "                ],\n",
    "                \"correct\": \"B\",\n",
    "                \"explanation\": \"Batch processing membantu mengoptimalkan penggunaan memory dan meningkatkan performance loading.\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"Business Intelligence\",\n",
    "                \"question\": \"Kategori AQI mana yang paling dominan dalam dataset kita?\",\n",
    "                \"options\": [\n",
    "                    \"A. Good (37.57%)\",\n",
    "                    \"B. Moderate (59.87%)\",\n",
    "                    \"C. Unhealthy (2.56%)\",\n",
    "                    \"D. Hazardous (0%)\"\n",
    "                ],\n",
    "                \"correct\": \"B\",\n",
    "                \"explanation\": \"Berdasarkan analisis, kategori Moderate mendominasi dengan 59.87% dari total readings.\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"Data Engineering\",\n",
    "                \"question\": \"Apa kegunaan utama dimension tables dalam star schema?\",\n",
    "                \"options\": [\n",
    "                    \"A. Menyimpan data transaksional\",\n",
    "                    \"B. Menyimpan data master/referensi untuk lookup\",\n",
    "                    \"C. Backup data fact table\", \n",
    "                    \"D. Menyimpan hasil aggregasi\"\n",
    "                ],\n",
    "                \"correct\": \"B\",\n",
    "                \"explanation\": \"Dimension tables menyimpan data master/referensi yang digunakan untuk lookup dan memberikan konteks pada fact table.\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"Data Analysis\",\n",
    "                \"question\": \"Berdasarkan data warehouse kita, berapa total sensor readings yang berhasil di-load?\",\n",
    "                \"options\": [\n",
    "                    \"A. 28,814 records\",\n",
    "                    \"B. 14,400 records\",\n",
    "                    \"C. 15,865 records\",\n",
    "                    \"D. 20,000 records\"\n",
    "                ],\n",
    "                \"correct\": \"B\",\n",
    "                \"explanation\": \"Setelah cleaning (menghapus duplikat), total 14,400 sensor readings berhasil di-load ke fact table.\"\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"Tools & Technology\",\n",
    "                \"question\": \"Database engine apa yang kita gunakan dalam tutorial ini?\",\n",
    "                \"options\": [\n",
    "                    \"A. PostgreSQL\",\n",
    "                    \"B. MySQL\",\n",
    "                    \"C. SQLite\",\n",
    "                    \"D. Oracle\"\n",
    "                ],\n",
    "                \"correct\": \"C\",\n",
    "                \"explanation\": \"Tutorial ini menggunakan SQLite sebagai database engine untuk kemudahan setup dan pembelajaran.\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def display_question(self, q_num, question):\n",
    "        print(f\"\\nüìù Pertanyaan {q_num}/10 - Kategori: {question['category']}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\n‚ùì {question['question']}\")\n",
    "        print()\n",
    "        for option in question['options']:\n",
    "            print(f\"   {option}\")\n",
    "        print()\n",
    "    \n",
    "    def get_answer(self):\n",
    "        while True:\n",
    "            answer = input(\"Pilihan Anda (A/B/C/D): \").strip().upper()\n",
    "            if answer in ['A', 'B', 'C', 'D']:\n",
    "                return answer\n",
    "            print(\"‚ùå Pilihan tidak valid. Gunakan A, B, C, atau D.\")\n",
    "    \n",
    "    def check_answer(self, user_answer, question):\n",
    "        self.total_questions += 1\n",
    "        if user_answer == question['correct']:\n",
    "            self.score += 1\n",
    "            print(\"‚úÖ BENAR!\")\n",
    "            print(f\"üí° {question['explanation']}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå SALAH! Jawaban yang benar adalah {question['correct']}\")\n",
    "            print(f\"üí° {question['explanation']}\")\n",
    "            return False\n",
    "    \n",
    "    def run_quiz(self, num_questions=10):\n",
    "        print(\"üß† KUIS DATA WAREHOUSE & ETL\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"üìö Test pemahaman Anda tentang konsep yang telah dipelajari!\")\n",
    "        print(\"üéØ 10 pertanyaan pilihan ganda\")\n",
    "        print(\"‚è∞ Tidak ada batasan waktu - ambil waktu untuk berpikir\")\n",
    "        print(\"\\nTekan Enter untuk memulai...\")\n",
    "        input()\n",
    "        \n",
    "        # Shuffle questions for variety\n",
    "        quiz_questions = random.sample(self.questions, min(num_questions, len(self.questions)))\n",
    "        \n",
    "        for i, question in enumerate(quiz_questions, 1):\n",
    "            self.display_question(i, question)\n",
    "            user_answer = self.get_answer()\n",
    "            self.check_answer(user_answer, question)\n",
    "            \n",
    "            if i < len(quiz_questions):\n",
    "                print(\"\\n\" + \"-\"*50)\n",
    "                input(\"Tekan Enter untuk pertanyaan berikutnya...\")\n",
    "        \n",
    "        self.show_results()\n",
    "    \n",
    "    def show_results(self):\n",
    "        percentage = (self.score / self.total_questions) * 100\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üèÜ HASIL KUIS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"üìä Skor Anda: {self.score}/{self.total_questions} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if percentage >= 90:\n",
    "            print(\"üåü EXCELLENT! Anda menguasai konsep data warehouse dengan sangat baik!\")\n",
    "            grade = \"A\"\n",
    "        elif percentage >= 80:\n",
    "            print(\"üëç VERY GOOD! Pemahaman Anda tentang data warehouse sudah bagus!\")\n",
    "            grade = \"B\"\n",
    "        elif percentage >= 70:\n",
    "            print(\"üëå GOOD! Anda memahami konsep dasar, tapi masih ada ruang untuk improvement.\")\n",
    "            grade = \"C\"\n",
    "        elif percentage >= 60:\n",
    "            print(\"üìñ NEED IMPROVEMENT. Review kembali materi dan praktik lebih banyak.\")\n",
    "            grade = \"D\"\n",
    "        else:\n",
    "            print(\"üìö NEED MORE STUDY. Silakan review ulang tutorial dan coba lagi.\")\n",
    "            grade = \"F\"\n",
    "        \n",
    "        print(f\"üéì Grade: {grade}\")\n",
    "        \n",
    "        # Recommendations based on score\n",
    "        print(f\"\\nüí° REKOMENDASI:\")\n",
    "        if percentage < 70:\n",
    "            print(\"‚Ä¢ Review kembali section konsep data warehouse\")\n",
    "            print(\"‚Ä¢ Praktik lebih banyak query SQL\")\n",
    "            print(\"‚Ä¢ Pahami kembali proses ETL step by step\")\n",
    "        elif percentage < 90:\n",
    "            print(\"‚Ä¢ Eksplorasi lebih advanced SQL queries\")\n",
    "            print(\"‚Ä¢ Pelajari best practices data warehouse design\")\n",
    "            print(\"‚Ä¢ Coba implementasi dengan dataset lain\")\n",
    "        else:\n",
    "            print(\"‚Ä¢ Explore real-time ETL dengan Apache Kafka\")\n",
    "            print(\"‚Ä¢ Learn cloud data warehouse (BigQuery, Snowflake)\")\n",
    "            print(\"‚Ä¢ Advanced analytics dan machine learning\")\n",
    "        \n",
    "        print(f\"\\nüéâ Terima kasih telah mengikuti kuis!\")\n",
    "        print(\"üí™ Keep learning and practicing data engineering!\")\n",
    "\n",
    "# Initialize the quiz\n",
    "print(\"üéÆ INTERACTIVE DATA WAREHOUSE QUIZ\")\n",
    "print(\"=\"*40)\n",
    "print(\"Kuis ini akan menguji pemahaman Anda tentang:\")\n",
    "print(\"‚Ä¢ Konsep Data Warehouse & Star Schema\")\n",
    "print(\"‚Ä¢ Proses ETL (Extract, Transform, Load)\")\n",
    "print(\"‚Ä¢ SQL Queries & Database Design\") \n",
    "print(\"‚Ä¢ Data Quality & Performance\")\n",
    "print(\"‚Ä¢ Business Intelligence Concepts\")\n",
    "\n",
    "quiz = DataWarehouseQuiz()\n",
    "print(f\"\\n‚úÖ Quiz sistem telah diinisialisasi!\")\n",
    "print(f\"üìö Tersedia {len(quiz.questions)} pertanyaan dalam berbagai kategori\")\n",
    "print(f\"\\nüí° Untuk memulai kuis, jalankan: quiz.run_quiz()\")\n",
    "print(f\"üí° Atau coba quiz singkat: quiz.run_quiz(5)  # untuk 5 pertanyaan saja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c282174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jalankan Kuis!\n",
    "print(\"üöÄ MEMULAI KUIS DATA WAREHOUSE\")\n",
    "print(\"=\"*40)\n",
    "print(\"Silakan pilih mode kuis yang Anda inginkan:\")\n",
    "print()\n",
    "print(\"üéØ Mode Kuis:\")\n",
    "print(\"1. Full Quiz (10 pertanyaan) - Durasi ~10 menit\")\n",
    "print(\"2. Quick Quiz (5 pertanyaan) - Durasi ~5 menit\") \n",
    "print(\"3. Practice Mode (3 pertanyaan) - Durasi ~3 menit\")\n",
    "print()\n",
    "\n",
    "# Uncomment salah satu baris di bawah untuk memulai kuis:\n",
    "\n",
    "# quiz.run_quiz(10)  # Full quiz - 10 pertanyaan\n",
    "# quiz.run_quiz(5)   # Quick quiz - 5 pertanyaan  \n",
    "# quiz.run_quiz(3)   # Practice mode - 3 pertanyaan\n",
    "\n",
    "print(\"üí° CARA MEMULAI KUIS:\")\n",
    "print(\"1. Uncomment salah satu baris di atas (hapus tanda #)\")\n",
    "print(\"2. Jalankan cell ini\")\n",
    "print(\"3. Ikuti instruksi yang muncul\")\n",
    "print()\n",
    "print(\"üìù CONTOH:\")\n",
    "print(\"   Hapus # pada baris: # quiz.run_quiz(5)\")\n",
    "print(\"   Menjadi: quiz.run_quiz(5)\")\n",
    "print()\n",
    "print(\"üéä Selamat mengerjakan kuis!\")\n",
    "print(\"üí™ Semoga sukses menguji pemahaman data warehouse Anda!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c53fe",
   "metadata": {},
   "source": [
    "### üìö **Kunci Jawaban & Pembahasan**\n",
    "\n",
    "Setelah mengerjakan kuis, Anda bisa review kunci jawaban dan pembahasan detail di bawah ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72aab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kunci Jawaban & Pembahasan Detail\n",
    "def show_answer_key():\n",
    "    print(\"üìã KUNCI JAWABAN & PEMBAHASAN LENGKAP\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, q in enumerate(quiz.questions, 1):\n",
    "        print(f\"\\nüìù Pertanyaan {i}: {q['category']}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"‚ùì {q['question']}\")\n",
    "        print(f\"‚úÖ Jawaban: {q['correct']}\")\n",
    "        print(f\"üí° Pembahasan: {q['explanation']}\")\n",
    "        \n",
    "        if i % 3 == 0 and i < len(quiz.questions):  # Pause every 3 questions\n",
    "            print(f\"\\n{'='*30} PAUSE {'='*30}\")\n",
    "            print(f\"üìä Progress: {i}/{len(quiz.questions)} pertanyaan\")\n",
    "    \n",
    "    print(f\"\\nüéì RINGKASAN MATERI PENTING:\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"üèóÔ∏è STAR SCHEMA:\")\n",
    "    print(\"   ‚Ä¢ Fact table di tengah (sensor readings)\")\n",
    "    print(\"   ‚Ä¢ Dimension tables di sekitar (sensors, locations, time)\")\n",
    "    print(\"   ‚Ä¢ Optimal untuk analytics dan reporting\")\n",
    "    \n",
    "    print(f\"\\nüîÑ ETL PROCESS:\")\n",
    "    print(\"   ‚Ä¢ Extract: Baca data dari sumber (CSV)\")\n",
    "    print(\"   ‚Ä¢ Transform: Bersihkan dan validasi data\")\n",
    "    print(\"   ‚Ä¢ Load: Masukkan ke warehouse dengan batch processing\")\n",
    "    \n",
    "    print(f\"\\nüìä DATA QUALITY:\")\n",
    "    print(\"   ‚Ä¢ Completeness: 100% untuk semua fields utama\")\n",
    "    print(\"   ‚Ä¢ Consistency: Standarisasi format dan tipe data\")\n",
    "    print(\"   ‚Ä¢ Accuracy: Validasi range values dan outliers\")\n",
    "    \n",
    "    print(f\"\\nüöÄ PERFORMANCE OPTIMIZATION:\")\n",
    "    print(\"   ‚Ä¢ Batch processing (1000 records per batch)\")\n",
    "    print(\"   ‚Ä¢ Database indexing pada foreign keys\")\n",
    "    print(\"   ‚Ä¢ Data normalization melalui dimensions\")\n",
    "    \n",
    "    print(f\"\\nüí° BUSINESS INSIGHTS:\")\n",
    "    print(\"   ‚Ä¢ 5 lokasi dengan distribusi data merata\")\n",
    "    print(\"   ‚Ä¢ AQI dominan kategori Moderate (59.87%)\")\n",
    "    print(\"   ‚Ä¢ Temperature range: 10.25¬∞C - 60.0¬∞C\")\n",
    "    print(\"   ‚Ä¢ Data coverage: 2 bulan (Jan-Feb 2024)\")\n",
    "\n",
    "# Show study guide\n",
    "def show_study_guide():\n",
    "    print(\"üìñ STUDY GUIDE UNTUK REVIEW\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    topics = {\n",
    "        \"Data Warehouse Fundamentals\": [\n",
    "            \"Definisi dan tujuan data warehouse\",\n",
    "            \"Perbedaan OLTP vs OLAP\",\n",
    "            \"Star schema vs Snowflake schema\",\n",
    "            \"Fact tables vs Dimension tables\"\n",
    "        ],\n",
    "        \"ETL Process\": [\n",
    "            \"Extract: Sumber data dan metode ekstraksi\",\n",
    "            \"Transform: Data cleaning, validation, standardization\",\n",
    "            \"Load: Batch processing, performance optimization\",\n",
    "            \"Error handling dan data quality checks\"\n",
    "        ],\n",
    "        \"SQL untuk Analytics\": [\n",
    "            \"Aggregate functions (COUNT, AVG, SUM)\",\n",
    "            \"GROUP BY dan HAVING clauses\", \n",
    "            \"Date/time functions (strftime, DATE)\",\n",
    "            \"JOIN operations antara fact dan dimensions\"\n",
    "        ],\n",
    "        \"Performance & Best Practices\": [\n",
    "            \"Database indexing strategy\",\n",
    "            \"Batch processing untuk large datasets\",\n",
    "            \"Memory management\",\n",
    "            \"Query optimization techniques\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for topic, subtopics in topics.items():\n",
    "        print(f\"\\nüéØ {topic}:\")\n",
    "        for subtopic in subtopics:\n",
    "            print(f\"   ‚Ä¢ {subtopic}\")\n",
    "    \n",
    "    print(f\"\\nüìö RECOMMENDED NEXT STEPS:\")\n",
    "    print(\"‚Ä¢ Practice dengan dataset yang lebih besar\")\n",
    "    print(\"‚Ä¢ Explore cloud data warehouses (BigQuery, Snowflake)\")\n",
    "    print(\"‚Ä¢ Learn Apache Airflow untuk ETL automation\")\n",
    "    print(\"‚Ä¢ Study real-time processing dengan Kafka/Spark\")\n",
    "\n",
    "print(\"üéì LEARNING RESOURCES\")\n",
    "print(\"=\"*30)\n",
    "print(\"Pilih resource yang ingin Anda akses:\")\n",
    "print()\n",
    "print(\"1Ô∏è‚É£ Kunci Jawaban & Pembahasan:\")\n",
    "print(\"   show_answer_key()\")\n",
    "print()\n",
    "print(\"2Ô∏è‚É£ Study Guide untuk Review:\")\n",
    "print(\"   show_study_guide()\")\n",
    "print()\n",
    "print(\"üí° Jalankan fungsi di atas untuk mengakses materi pembelajaran!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f02f81",
   "metadata": {},
   "source": [
    "### üèÖ **Quiz Challenge & Achievements**\n",
    "\n",
    "Bagian ini berisi challenge tambahan untuk menguji kemampuan advanced dan sistem achievements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Quiz Challenges\n",
    "class AdvancedChallenge:\n",
    "    def __init__(self):\n",
    "        self.challenges = {\n",
    "            \"SQL Master\": {\n",
    "                \"description\": \"Tulis query untuk menemukan sensor dengan performa terbaik\",\n",
    "                \"task\": \"\"\"\n",
    "Tantangan: Tulis SQL query untuk menemukan:\n",
    "1. Sensor dengan reading paling konsisten (variasi temperature terkecil)\n",
    "2. Lokasi dengan kualitas udara terbaik rata-rata\n",
    "3. Hari dalam seminggu dengan suhu tertinggi\n",
    "\n",
    "Gunakan fungsi warehouse.execute_query() untuk menjalankan query Anda!\n",
    "                \"\"\",\n",
    "                \"points\": 50\n",
    "            },\n",
    "            \n",
    "            \"Data Detective\": {\n",
    "                \"description\": \"Temukan anomali dan pattern tersembunyi dalam data\",\n",
    "                \"task\": \"\"\"\n",
    "Tantangan Detective:\n",
    "1. Identifikasi reading yang terlihat mencurigakan (outliers)\n",
    "2. Temukan korelasi antara waktu dan kualitas udara\n",
    "3. Analisis apakah ada sensor yang bermasalah\n",
    "\n",
    "Tip: Gunakan statistical functions dan conditional logic!\n",
    "                \"\"\",\n",
    "                \"points\": 75\n",
    "            },\n",
    "            \n",
    "            \"Business Analyst\": {\n",
    "                \"description\": \"Buat insights bisnis dari data warehouse\",\n",
    "                \"task\": \"\"\"\n",
    "Tantangan Business:\n",
    "1. Rekomendasikan lokasi terbaik untuk kantor baru berdasarkan AQI\n",
    "2. Tentukan jam operasional optimal berdasarkan kondisi lingkungan\n",
    "3. Buat forecast sederhana untuk bulan berikutnya\n",
    "\n",
    "Presentasikan findings dengan visualisasi!\n",
    "                \"\"\",\n",
    "                \"points\": 100\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def show_challenges(self):\n",
    "        print(\"üèÜ ADVANCED CHALLENGES\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"Complete these challenges to earn achievement points!\")\n",
    "        print()\n",
    "        \n",
    "        for name, challenge in self.challenges.items():\n",
    "            print(f\"üéØ {name} ({challenge['points']} points)\")\n",
    "            print(f\"üìã {challenge['description']}\")\n",
    "            print(f\"üí° {challenge['task']}\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        print(\"üèÖ ACHIEVEMENT LEVELS:\")\n",
    "        print(\"‚Ä¢ ü•â Bronze (50+ points): SQL Apprentice\")\n",
    "        print(\"‚Ä¢ ü•à Silver (125+ points): Data Analyst\") \n",
    "        print(\"‚Ä¢ ü•á Gold (225+ points): Data Warehouse Expert\")\n",
    "\n",
    "# Quick Assessment Questions\n",
    "def quick_assessment():\n",
    "    print(\"‚ö° QUICK ASSESSMENT - 5 MENIT CHALLENGE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    questions = [\n",
    "        {\n",
    "            \"q\": \"Sebutkan 3 keuntungan utama star schema!\",\n",
    "            \"type\": \"open\",\n",
    "            \"points\": 10\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"Mengapa kita perlu data cleaning dalam ETL?\",\n",
    "            \"type\": \"open\", \n",
    "            \"points\": 10\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"Tulis query SQL untuk menemukan temperature tertinggi per lokasi!\",\n",
    "            \"type\": \"code\",\n",
    "            \"points\": 20\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"Bagaimana cara mengoptimalkan performance loading data?\",\n",
    "            \"type\": \"open\",\n",
    "            \"points\": 15\n",
    "        },\n",
    "        {\n",
    "            \"q\": \"Apa perbedaan dimension table dan fact table?\",\n",
    "            \"type\": \"open\",\n",
    "            \"points\": 15\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"üìù PERTANYAAN QUICK ASSESSMENT:\")\n",
    "    print(\"(Jawab dalam pikiran Anda, atau tulis di cell terpisah)\")\n",
    "    print()\n",
    "    \n",
    "    total_points = 0\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"{i}. {q['q']} ({q['points']} poin)\")\n",
    "        total_points += q['points']\n",
    "    \n",
    "    print(f\"\\nüéØ Total Points Available: {total_points}\")\n",
    "    print(\"‚è∞ Time Limit: 5 minutes\")\n",
    "    print(\"üí° Self-evaluate your answers based on tutorial content!\")\n",
    "\n",
    "# Initialize challenges\n",
    "challenge = AdvancedChallenge()\n",
    "\n",
    "print(\"üéÆ CHALLENGE MODE ACTIVATED\")\n",
    "print(\"=\"*35)\n",
    "print(\"Choose your challenge level:\")\n",
    "print()\n",
    "print(\"1Ô∏è‚É£ Advanced Technical Challenges:\")\n",
    "print(\"   challenge.show_challenges()\")\n",
    "print()\n",
    "print(\"2Ô∏è‚É£ Quick 5-Minute Assessment:\")\n",
    "print(\"   quick_assessment()\")\n",
    "print()\n",
    "print(\"üèÜ Earn points and unlock achievements!\")\n",
    "print(\"üí™ Challenge yourself to become a Data Warehouse Expert!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
