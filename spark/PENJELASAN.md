# Penjelasan Lengkap IoT Streaming Pipeline

## ðŸŽ¯ Tujuan Pembelajaran

Proyek ini mengajarkan konsep **streaming data pipeline** yang banyak digunakan di industri untuk:
- Real-time monitoring (IoT devices, aplikasi mobile, web analytics)
- Fraud detection di financial services
- Recommendation systems
- Log processing & observability

## ðŸ“Š Arsitektur Detail

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PRODUCER (IoT Simulator)                     â”‚
â”‚                         src/producer.py                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Simulasi 8 devices IoT                                        â”‚
â”‚ â€¢ Generate random metrics: temp, humidity, battery              â”‚
â”‚ â€¢ Publish ke Kafka topic "iot-metrics"                          â”‚
â”‚ â€¢ Rate: 4 messages/second per device = 32 msg/s total          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ JSON over TCP
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA (Message Broker)                        â”‚
â”‚                    Docker: apache/kafka:3.8.0                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Topic: iot-metrics                                              â”‚
â”‚ â€¢ Partitions: 3 (parallel processing)                           â”‚
â”‚ â€¢ Retention: default 7 days                                     â”‚
â”‚ â€¢ Port: 9092                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Consumer reads stream
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SPARK STREAMING (Processing Engine)                 â”‚
â”‚                    src/spark_streaming.py                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Read from Kafka (structured streaming)                       â”‚
â”‚ 2. Parse JSON payload                                           â”‚
â”‚ 3. Convert timestamp (epoch ms â†’ TimestampType)                 â”‚
â”‚ 4. Apply watermark (1 min for late data)                        â”‚
â”‚ 5. Window aggregation (10 second tumbling)                      â”‚
â”‚    - Count messages                                             â”‚
â”‚    - AVG(temperature), AVG(humidity)                            â”‚
â”‚    - MIN(battery), MAX(battery)                                 â”‚
â”‚    GROUP BY device_id, window                                   â”‚
â”‚ 6. Write results every 5 seconds                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Batch write (foreachBatch)
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. data/aggregates/latest.json (atomic write)                  â”‚
â”‚    â””â”€> Dashboard reads this file                                â”‚
â”‚                                                                  â”‚
â”‚ 2. data/aggregates/parquet/ (historical)                        â”‚
â”‚    â””â”€> Partitioned by device_id                                 â”‚
â”‚    â””â”€> For later analytics / ML                                 â”‚
â”‚                                                                  â”‚
â”‚ 3. data/aggregates/_checkpoints/                                â”‚
â”‚    â””â”€> Spark streaming state & offset tracking                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ HTTP polling (2s interval)
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DASHBOARD (Visualization)                       â”‚
â”‚                   src/dashboard/app.py                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Flask server (port 5000)                                        â”‚
â”‚                                                                  â”‚
â”‚ Routes:                                                          â”‚
â”‚ â€¢ GET /         â†’ serve HTML (Chart.js)                         â”‚
â”‚ â€¢ GET /metrics  â†’ read latest.json, return as JSON             â”‚
â”‚                                                                  â”‚
â”‚ Frontend (index.html):                                          â”‚
â”‚ â€¢ Poll /metrics every 2 seconds                                 â”‚
â”‚ â€¢ Extract latest window data                                    â”‚
â”‚ â€¢ Render bar charts (temperature, humidity per device)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              User's Web Browser
              http://127.0.0.1:5000
```

## ðŸ”„ Data Flow Example

### 1. Producer generates message
```json
{
  "device_id": "device-abc123",
  "ts": 1730638574123,        // epoch milliseconds
  "temperature": 24.7,         // Celsius
  "humidity": 55.3,            // %
  "battery": 82.5,             // %
  "status": "ok",              // ok|warn|error
  "location": "lab"            // floor-1|floor-2|lab|office
}
```

### 2. Kafka stores in topic partition
- Key: `device-abc123` (ensures messages from same device go to same partition)
- Value: JSON string
- Offset: auto-incremented (e.g., 12345)

### 3. Spark reads & processes
```python
# Input: raw Kafka messages
# Parse JSON, extract fields
# Convert ts â†’ event_time (timestamp)

# Window: 10 seconds
# Example: 14:30:00 - 14:30:10

# Aggregate per device in window:
{
  "device_id": "device-abc123",
  "window_start": "2024-11-03T14:30:00",
  "window_end": "2024-11-03T14:30:10",
  "count": 40,                      // 4 msg/s Ã— 10s
  "avg_temperature": 24.52,
  "avg_humidity": 54.87,
  "min_battery": 80.1,
  "max_battery": 85.3
}
```

### 4. Write to latest.json
```json
{
  "updated_at": 1730638580,
  "records": [
    {
      "device_id": "device-abc123",
      "window_start": "2024-11-03T14:30:00",
      "window_end": "2024-11-03T14:30:10",
      "count": 40,
      "avg_temperature": 24.52,
      "avg_humidity": 54.87,
      "min_battery": 80.1,
      "max_battery": 85.3
    },
    // ... 7 more devices
  ]
}
```

### 5. Dashboard renders
- Polls `/metrics` â†’ gets JSON above
- Extract latest window (newest `window_start`)
- Group by device
- Render 2 bar charts

## âš™ï¸ Konfigurasi Penting

### Spark Streaming Window
```python
# src/spark_streaming.py, line ~92
.groupBy(
    F.window(F.col("event_time"), "10 seconds"),  # Window size
    F.col("device_id"),
)
```

**Apa itu window?**
- Tumbling window: non-overlapping, fixed size
- Event time: berdasarkan timestamp data, bukan processing time
- 10 detik â†’ setiap 10s, hitung agregat baru

**Mengapa tumbling?**
- Sederhana untuk dipelajari
- Cocok untuk dashboard yang update berkala
- Alternatif: sliding window (overlapping)

### Watermark
```python
# src/spark_streaming.py, line ~89
.withWatermark("event_time", "1 minute")
```

**Apa itu watermark?**
- Toleransi untuk late-arriving data
- Jika message datang > 1 menit late, discard
- Spark tidak tunggu selamanya untuk data terlambat

**Contoh:**
- Window: 14:30:00 - 14:30:10
- Watermark: 1 menit
- Spark akan finalize window setelah melihat data dengan timestamp 14:31:10

### Trigger Interval
```python
# src/spark_streaming.py, line ~124
.trigger(processingTime="5 seconds")
```

**Apa artinya?**
- Spark check Kafka setiap 5 detik
- Proses batch baru (micro-batch)
- Trade-off: lebih kecil = lebih real-time, tapi overhead lebih tinggi

## ðŸ§ª Eksperimen untuk Belajar

### 1. Ubah window size
Ganti `"10 seconds"` â†’ `"30 seconds"` di `spark_streaming.py`
**Efek:** Agregat per 30s, lebih smooth tapi kurang real-time

### 2. Tambah device count
```zsh
python -m src.producer --devices 20 --rate 10.0
```
**Efek:** 20 devices Ã— 10 msg/s = 200 msg/s throughput

### 3. Tambah aggregation function
```python
# di spark_streaming.py, tambah:
F.stddev("temperature").alias("stddev_temp"),
```
**Efek:** Dashboard bisa show variability

### 4. Late data simulation
Edit `producer.py`, tambah random delay:
```python
import random
msg["ts"] = int((time.time() - random.uniform(0, 120)) * 1000)
```
**Efek:** Lihat watermark behavior (late data discarded)

## ðŸ“ˆ Metrics yang Bisa Dimonitor

### Kafka
- **Throughput:** messages/second
  ```zsh
  # Check via Kafka UI: http://localhost:8080
  ```

### Spark
- **Processing time:** berapa lama proses 1 batch?
- **Input rate:** records/second
- **Watermark:** current watermark value
  ```
  # Check di Spark logs (terminal 2)
  ```

### Dashboard
- **Latency:** dari message produced sampai tampil di chart
- **Update frequency:** seberapa sering data refresh?

## ðŸŽ“ Konsep Lanjutan

Setelah paham proyek ini, belajar:

1. **Exactly-once semantics**
   - Kafka transactions
   - Spark idempotent writes

2. **State management**
   - Session windows
   - Stream-stream joins

3. **Backpressure handling**
   - Kafka consumer lag
   - Spark adaptive batch sizing

4. **Monitoring & alerting**
   - Prometheus + Grafana
   - Dead letter queues

5. **Production deployment**
   - Kubernetes (Kafka, Spark on K8s)
   - Auto-scaling
   - High availability

## ðŸ”— Referensi Belajar

- [Kafka in Action (book)](https://www.manning.com/books/kafka-in-action)
- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Event Time vs Processing Time](https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/)

---

**Happy learning! Jika ada error, cek PANDUAN.md bagian Troubleshooting.** ðŸš€
